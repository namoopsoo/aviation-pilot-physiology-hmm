{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/sorted_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5278920581455526, 4.579843668496, 2.098924312471526, 4.015046017496413]\n",
      "{0: 0.5898470233582309, 1: 0.027880056445732652, 2: 0.33322934162364914, 3: 0.0490435785723873}\n"
     ]
    }
   ],
   "source": [
    "n_classes = 4\n",
    "\n",
    "counts = {'A': 325198, 'C': 183718, 'D': 27039, 'B': 15371}\n",
    "total_count = sum(counts.values())\n",
    "\n",
    "weightsArray =[]\n",
    "for i, _class in enumerate(['A', 'B', 'C', 'D']):\n",
    "    weightsArray.append(math.log(total_count/max(counts[_class],1))+1)\n",
    "\n",
    "# approach from stack overflow, \n",
    "# https://datascience.stackexchange.com/questions/12886/tensorflow-adjusting-cost-function-for-imbalanced-data?newreg=1ec2de91ae154cd0ad5c459b3a2adfaf\n",
    "print (weightsArray)\n",
    "\n",
    "# or just actual weights... uhmmm.\n",
    "print({i: counts[x]/total_count for i, x in enumerate(['A', 'B', 'C', 'D'])})\n",
    "\n",
    "# Whwoops, first go I had created weights which were proportional \n",
    "# instead of inversely proportional.\n",
    "# [1.5278920581455526, 2.098924312471526, 4.015046017496413, 4.579843668496]\n",
    "# {0: 0.5898470233582309, 1: 0.027880056445732652, 2: 0.33322934162364914, 3: 0.0490435785723873}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://datascience.stackexchange.com/questions/12886/tensorflow-adjusting-cost-function-for-imbalanced-data?newreg=1ec2de91ae154cd0ad5c459b3a2adfaf\n",
    "# \n",
    "class_weight = tf.constant(weightsArray)\n",
    "weighted_logits = tf.mul(pred, class_weight)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(weighted_logits, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# And that question's answer,...\n",
    "# Take the cost like normal\n",
    "error = tf.nn.softmax_cross_entropy_with_logits(pred, y)\n",
    "\n",
    "# Scale the cost by the class weights\n",
    "scaled_error = tf.mul(error, class_weight)\n",
    "\n",
    "# Reduce\n",
    "cost = tf.reduce_mean(scaled_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.74366838 1.74366838 1.74366838]\n",
      " [0.74366838 1.74366838 1.74366838]\n",
      " [0.74366838 1.74366838 1.74366838]]\n"
     ]
    }
   ],
   "source": [
    "# Construct a `Session` to execute the graph.\n",
    "# sess = tf.Session()\n",
    "# Execute the graph and store the value that `e` represents in `result`.\n",
    "# result = sess.run(tensor)\n",
    "\n",
    "# init_op = tf.global_variables_initializer()   # ?\n",
    "class_weights_d = {0: 0.5898470233582309,\n",
    "                     1: 0.027880056445732652,\n",
    "                     2: 0.33322934162364914,\n",
    "                     3: 0.0490435785723873}\n",
    "class_weights_v = tf.constant(np.array([class_weights_d[k] for k in range(3)]))    \n",
    "#class_weights_v = [1.5278920581455526, 4.579843668496, 2.098924312471526, 4.015046017496413]\n",
    "class_weights_v = [[1],[1],[1]]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    target = np.array([[1.,0.,0.,0.], # target.\n",
    "                       [0.,1.,0.,0.],\n",
    "                       [0.,0.,1.,0.],\n",
    "                      ])\n",
    "    prediction = np.array([[1.,0.,0.,0.], # prediction\n",
    "                           [1.,0.,0.,0.],\n",
    "                           [1.,0.,0.,0.],\n",
    "                           ])\n",
    "    error = tf.nn.softmax_cross_entropy_with_logits(logits=prediction, \n",
    "                                                    labels=target)\n",
    "\n",
    "    # (labels=target,  logits=output)\n",
    "    \n",
    "    scaled_error = tf.multiply(error, class_weights_v)\n",
    "    result = sess.run(scaled_error)    \n",
    "    # acc, l, soft_max_a = sess.run([accuracy, loss, a], feed_dict={x: X_t, y: Y_t})\n",
    "    \n",
    "#     tf.add\n",
    "print(result)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43865058, 0.04861357, 0.58104147])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bake_model4_may18(x_train, y_train, x_test, y_test, epochs=1, **kwargs):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2,\n",
    "                   input_shape=(None, 1)))\n",
    "\n",
    "    # 4 because 'A', 'B', 'C', 'D'.\n",
    "    model.add(Dense(4))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', # unbalanced_categorical_crossentropy,\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # ðŸ¤” model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # initial_state ... for the LSTM , hmm\n",
    "\n",
    "    monitor = EarlyStopping(monitor='val_loss',\n",
    "                            min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "    print('Train...', timestamp())\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#arguments_8\n",
    "    # - hmm so fit() can take a generator sometimes.\n",
    "    # - use_multiprocessing=True \n",
    "    model.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
    "              callbacks=[monitor], verbose=2, epochs=epochs)\n",
    "\n",
    "    return model\n",
    "\n",
    "# forking from here:\n",
    "# https://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py#L3527\n",
    "def unbalanced_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n",
    "    \"\"\"Categorical crossentropy between an output tensor and a target tensor.\n",
    "    # Arguments\n",
    "        target: A tensor of the same shape as `output`.\n",
    "        output: A tensor resulting from a softmax\n",
    "            (unless `from_logits` is True, in which\n",
    "            case `output` is expected to be the logits).\n",
    "        from_logits: Boolean, whether `output` is the\n",
    "            result of a softmax, or is a tensor of logits.\n",
    "        axis: Int specifying the channels axis. `axis=-1`\n",
    "            corresponds to data format `channels_last`,\n",
    "            and `axis=1` corresponds to data format\n",
    "            `channels_first`.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    # Raises\n",
    "        ValueError: if `axis` is neither -1 nor one of\n",
    "            the axes of `output`.\n",
    "    \"\"\"\n",
    "    output_dimensions = list(range(len(output.get_shape())))\n",
    "    if axis != -1 and axis not in output_dimensions:\n",
    "        raise ValueError(\n",
    "            '{}{}{}'.format(\n",
    "                'Unexpected channels axis {}. '.format(axis),\n",
    "                'Expected to be -1 or one of the axes of `output`, ',\n",
    "                'which has {} dimensions.'.format(len(output.get_shape()))))\n",
    "        \n",
    "    # Pre-computed Hard-coded class_weights...\n",
    "    class_weights = {0: 0.5898470233582309,\n",
    "                     1: 0.027880056445732652,\n",
    "                     2: 0.33322934162364914,\n",
    "                     3: 0.0490435785723873}\n",
    "    # Note: tf.nn.softmax_cross_entropy_with_logits\n",
    "    # expects logits, Keras expects probabilities.\n",
    "    if not from_logits:\n",
    "        # scale preds so that the class probas of each sample sum to 1\n",
    "        output /= tf.reduce_sum(output, axis, True)\n",
    "        # manual computation of crossentropy\n",
    "        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)\n",
    "        output = tf.clip_by_value(output, _epsilon, 1. - _epsilon)\n",
    "        return - tf.reduce_sum(target * tf.log(output), axis)\n",
    "    else:\n",
    "        error = tf.nn.softmax_cross_entropy_with_logits(labels=target,\n",
    "                                                       logits=output)\n",
    "    \n",
    "        # Scale by class\n",
    "        class_weight = \n",
    "        return  tf.mul(error, class_weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# ok ran model-4 on Sagemaker notebook w/ one change, making use of the categorical \n",
    "restored_model4 = keras.models.load_model('models/2019-05-19T001217-UTC-model-4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start building training set 2019-05-19 00:08:20 EST\n",
      "Start building testing set 2019-05-19 00:08:26 EST\n"
     ]
    }
   ],
   "source": [
    "# do preds... get confusion..\n",
    "outdata = make_data(df, crews={'training': [1],\n",
    "                        'test': [2]},\n",
    "              sequence_window=256, percent_of_data=1,\n",
    "             feature_cols={'r': simple_scaler})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56min 23s, sys: 21min 7s, total: 1h 17min 30s\n",
      "Wall time: 21min 10s\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "%time preds, preds_classes = do_get_preds(outdata, restored_model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/tensorflow/python/ops/confusion_matrix.py:193: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "confusion_yea = do_confusiony(outdata, preds_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[325198,      0,      0,      0],\n",
       "       [ 15371,      0,      0,      0],\n",
       "       [183718,      0,      0,      0],\n",
       "       [ 27039,      0,      0,      0]], dtype=int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_yea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "encode_class = np.vectorize(lambda x: {'A': 0,\n",
    "                                      'B': 1,\n",
    "                                      'C': 2,\n",
    "                                      'D': 3}.get(x))\n",
    "\n",
    "decode_class = np.vectorize(lambda x: {0: 'A',\n",
    "                                      1: 'B',\n",
    "                                      2: 'C',\n",
    "                                      3: 'D'}.get(x))\n",
    "\n",
    "simple_scaler = lambda x, a: x*a \n",
    "\n",
    "def timestamp():\n",
    "    return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S EST')\n",
    "\n",
    "def do_standard_scaling(df, cols, scalar_dict=None):\n",
    "    if scalar_dict is None:    \n",
    "        scalar_dict = {col: StandardScaler().fit(df[[col]]) for col in cols}\n",
    "        \n",
    "    for col in cols:\n",
    "        df[col + '_scaled'] = np.resize(\n",
    "            scalar_dict[col].transform(df[[col]]),\n",
    "            (df.shape[0],))\n",
    "    \n",
    "    return scalar_dict, df\n",
    "\n",
    "\n",
    "def make_data(df, crews={'training': [1],\n",
    "                        'test': [2]},\n",
    "              sequence_window=256, percent_of_data=100,\n",
    "             feature_cols={'r': 'standard_scaler'}):\n",
    "\n",
    "    # current sorted as ['crew', 'experiment', 'time']\n",
    "    [0, 1] # each seat\n",
    "    ['CA', 'DA', 'SS'] # experiment\n",
    "    \n",
    "    sort_cols = ['crew', 'seat', 'experiment', 'time']\n",
    "    target_col = 'event'\n",
    "    \n",
    "    what_cols = sort_cols + list(feature_cols) + [target_col]\n",
    "\n",
    "    # Training\n",
    "    traindf = df[df.crew.isin(crews['training'])][what_cols].sort_values(\n",
    "        by=sort_cols).copy()\n",
    "    \n",
    "    scalar_dict, _ = do_standard_scaling(traindf, ['r'])\n",
    "    \n",
    "    print('Start building training set', timestamp())\n",
    "    x_train, y_train = get_windows(traindf, ['r_scaled', 'event'],\n",
    "                                   sequence_window,\n",
    "                                  percent_of_data=percent_of_data)\n",
    "    \n",
    "    # Testing\n",
    "    testdf = df[df.crew.isin(crews['test'])][what_cols].sort_values(\n",
    "        by=sort_cols).copy()\n",
    "\n",
    "    _, _ = do_standard_scaling(testdf, ['r'], scalar_dict)\n",
    "    \n",
    "    \n",
    "    print('Start building testing set', timestamp())\n",
    "    x_test, y_test = get_windows(testdf, ['r_scaled', 'event'],\n",
    "                                 sequence_window,\n",
    "                                 percent_of_data=percent_of_data)\n",
    "\n",
    "\n",
    "    outdata = {\n",
    "        \"x_train\": x_train,\n",
    "        \"y_train\": reshape_y(encode_class(y_train), 4), # y_train,\n",
    "        \"x_test\": x_test,\n",
    "        \"y_test\": reshape_y(encode_class(y_test), 4), # y_test\n",
    "        \"y_train_original\": y_train,\n",
    "        \"y_test_original\": y_test,\n",
    "        \"traindf\": traindf,\n",
    "        \"testdf\": testdf,}\n",
    "    metadata = {\n",
    "        \"metadata\": {\n",
    "            \"output\": {\n",
    "                \"shapes\": {k: outdata[k].shape for k in list(outdata)},\n",
    "                \"Counter(outdata['y_train_original'])\":\n",
    "                dict(Counter(y_train)),\n",
    "                \"Counter(outdata['y_test_original'])\":\n",
    "                dict(Counter(y_test)),},\n",
    "            \"input\": {\"kwargs\": {\n",
    "                \"crews\": crews,\n",
    "                \"percent_of_data\": percent_of_data,\n",
    "                \"sequence_window\": sequence_window,\n",
    "                \"feature_cols\": list(feature_cols)}},\n",
    "            \"data_ts\": timestamp()\n",
    "        }}\n",
    "            \n",
    "    return {**outdata, **metadata}\n",
    "    \n",
    "    \n",
    "def runner(df):\n",
    "    print('Start make_data', timestamp())\n",
    "    outdata = make_data(df, crews={'training': [1],\n",
    "                        'test': [2]},\n",
    "              sequence_window=256, percent_of_data=1,\n",
    "             feature_cols={'r': simple_scaler})\n",
    "    \n",
    "    validate_data(outdata)\n",
    "\n",
    "    print('Start bake_model', timestamp())\n",
    "    model = bake_model18(**outdata, epochs=2)\n",
    "    return outdata, model\n",
    "\n",
    "def validate_data(data):\n",
    "    assert len(Counter(data['y_train_original'])) > 1\n",
    "    assert len(Counter(data['y_test_original'])) > 1\n",
    "  \n",
    "    \n",
    "def get_windows(df, cols, window_size, percent_of_data=100):\n",
    "    \n",
    "    whats_proportion_index = lambda x, y: round(x*y)\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    choices = (df.crew.unique().tolist(), [0, 1], ['CA', 'DA', 'SS'])\n",
    "    for crew, seat, experiment in itertools.product(*choices):\n",
    "        query = (df.crew == crew)&(df.seat == seat)&(df.experiment == experiment)\n",
    "        thisdf = df[query][cols]\n",
    "        X_i, Y_i = to_sequences(thisdf.values, window_size)\n",
    "        X.append(X_i[:\n",
    "                     whats_proportion_index(\n",
    "                         X_i.shape[0],\n",
    "                         percent_of_data)])\n",
    "        Y.append(Y_i[:\n",
    "                     whats_proportion_index(\n",
    "                        Y_i.shape[0],\n",
    "                        percent_of_data)])\n",
    "        \n",
    "    return np.concatenate(X), np.concatenate(Y)\n",
    "\n",
    "# Borrowing parts of this func from \n",
    "# https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class10_lstm.ipynb\n",
    "def to_sequences(obs, seq_size, incols=[0], outcols=[1]):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(obs)-seq_size-1):\n",
    "        #print(i)\n",
    "        window = obs[i:(i+seq_size)][..., 0]\n",
    "        after_window = obs[i+seq_size, 1] # FIXME :off by 1 error here?\n",
    "        # window = [[x] for x in window]\n",
    "\n",
    "        x.append(window)\n",
    "        y.append(after_window)\n",
    "        \n",
    "    xarr = np.array(x)\n",
    "    yarr = np.array(y)\n",
    "    return (np.resize(xarr, xarr.shape + (1,)),\n",
    "            yarr)\n",
    "    \n",
    "    \n",
    "\n",
    "def reshape_y(y, num_cols):\n",
    "\n",
    "    # y = np.array([1,2,3,2,3,1],dtype=np.int32)\n",
    "\n",
    "    # Convert y2 to dummy variables\n",
    "    y2 = np.zeros((y.shape[0], num_cols), dtype=np.float32)\n",
    "    y2[np.arange(y.shape[0]), y] = 1.0\n",
    "    return y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_get_preds(outdata, model):\n",
    "    preds = model.predict(outdata['x_test'])\n",
    "    preds_classes = np.argmax(preds, axis=1)\n",
    "    # print('Counter predict classes', Counter(preds_classes))\n",
    "    # print('original', Counter(outdata['y_test_original']))\n",
    "    # \n",
    "    return preds, preds_classes\n",
    "    # \n",
    "    # \n",
    "    # preds, preds_classes = do_get_preds_and_confusion(outdata, model)\n",
    "    \n",
    "def do_confusiony(outdata, preds_classes):\n",
    "    tensor = tf.confusion_matrix( \n",
    "    encode_class(outdata['y_test_original']),# labels\n",
    "    preds_classes, # predictions\n",
    "    num_classes=4)\n",
    "\n",
    "    # Construct a `Session` to execute the graph.\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Execute the graph and store the value that `e` represents in `result`.\n",
    "        result = sess.run(tensor)\n",
    "        # print(result)\n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_s3_resource():\n",
    "    s3 = boto3.resource('s3',\n",
    "            # aws_access_key_id=os.getenv('S3_BLOG_UPLOAD_ACCESS_KEY'),\n",
    "            # aws_secret_access_key=os.getenv('S3_BLOG_UPLOAD_SECRET'),\n",
    "            region_name='us-east-1')\n",
    "    return s3\n",
    "\n",
    "\n",
    "def write_s3_file(bucket_name, s3_filename, content):\n",
    "    # assert isinstance(content, basestring)\n",
    "    s3conn = make_s3_resource()\n",
    "    # try:\n",
    "    s3conn.Object(bucket_name, s3_filename).put(\n",
    "            Body=content)\n",
    "    # return True\n",
    "    # except botocore.exceptions.ClientError as e:\n",
    "    # return False\n",
    "\n",
    "\n",
    "def read_s3_file(bucket_name, s3_filename):\n",
    "    s3conn = make_s3_resource()\n",
    "    # try:\n",
    "    return s3conn.Object(bucket_name, s3_filename).get()[\"Body\"].read()\n",
    "    # except botocore.exceptions.ClientError as e:\n",
    "\n",
    "\n",
    "def s3_csv_to_df(bucket_name, s3_filename):\n",
    "    blah = read_s3_file(bucket_name, s3_filename)\n",
    "    foo = StringIO(blah.decode(\"utf-8\"))\n",
    "    return pd.read_csv(foo)\n",
    "\n",
    "def big_s3_csv_to_df(bucket_name, s3_filename_prefix, suffixes):\n",
    "    filenames = [s3_filename_prefix + suff\n",
    "            for suff in suffixes]\n",
    "    # return filenames\n",
    "    parts = [read_s3_file(bucket_name, s3_filename) \n",
    "            for s3_filename in filenames ]\n",
    "    blah = functools.reduce(lambda x, y: x+y, parts)\n",
    "    foo = StringIO(blah.decode(\"utf-8\"))\n",
    "    return pd.read_csv(foo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
