

```python

from importlib import reload
import os
import pandas as pd
from io import StringIO
import itertools
import ipdb
import datetime
from collections import Counter

import h5py
import json
import tensorflow as tf
from tensorflow import keras

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import joblib
print(tf.__version__)

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Embedding
from keras.layers import LSTM

from keras.callbacks import EarlyStopping

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
import mytf.s3utils as msu
import mytf.utils as mu
import mytf.validation as mv
import mytf.plot as mp
```

    1.14.0


    Using TensorFlow backend.



```python
tf.enable_eager_execution()
```


```python
# In this notebook, going to do some plotting,
# associated with 2019-12-28-two.ipynb
# which is still running a model train..
```


```python
workdir = 'history/2019-12-29T000509Z'
```


```python
# Look at a most recent train loss plot so far..
historydir = 'history'
with open(f'{workdir}/epoch_002_batch_00910_train_loss_history.json') as fd:
    losshistory = json.load(fd)
    
plt.plot(losshistory) 
plt.title('Train xentropy logloss on epoch=2,batch=910')
```




    Text(0.5, 1.0, 'Train xentropy logloss on epoch=2,batch=910')




![png](2019-12-28-two-plot_files/2019-12-28-two-plot_4_1.png)



```python
# Look at a most recent train loss plot so far..
historydir = 'history'
with open(f'{workdir}/epoch_009_batch_01090_train_loss_history.json') as fd:
    losshistory = json.load(fd)
    
plt.plot(losshistory) 
plt.title('Train xentropy logloss on epoch=2,batch=910')
```




    Text(0.5, 1.0, 'Train xentropy logloss on epoch=2,batch=910')




![png](2019-12-28-two-plot_files/2019-12-28-two-plot_5_1.png)



```python

```


```python
ts = '2019-12-29T000509Z'
with open(f'{workdir}/epoch_006_batch_00600_validation_losses.json') as fd:
    batch_losses_vec = json.load(fd)
```


```python
batch_losses_vec
```




    {'batch_losses_vec': [[1.4803270101547241,
       1.0513440370559692,
       1.534023404121399,
       1.5512949228286743],
      [1.233873724937439,
       1.073665976524353,
       1.5134793519973755,
       1.574166178703308],
      [1.2491341829299927,
       0.9332689642906189,
       2.007120370864868,
       1.4428625106811523],
      [1.9173550605773926,
       0.39332115650177,
       2.0984182357788086,
       2.1318118572235107],
      [3.0079331398010254,
       0.2558949589729309,
       3.298619031906128,
       2.9812724590301514],
      [3.466482400894165,
       0.21102771162986755,
       3.9664459228515625,
       3.2732765674591064],
      [4.007164001464844,
       0.1189667358994484,
       4.4724273681640625,
       3.975036859512329],
      [5.425018787384033,
       0.053067490458488464,
       5.275328159332275,
       5.119890213012695],
      [5.590909481048584,
       0.04763505607843399,
       5.928332805633545,
       5.279109001159668],
      [5.999654769897461,
       0.03288821876049042,
       5.86484956741333,
       5.772319316864014],
      [5.772192001342773,
       0.047110628336668015,
       5.9548187255859375,
       5.369383335113525],
      [6.79311466217041,
       0.01659901812672615,
       6.213584899902344,
       6.390729904174805],
      [6.918260097503662,
       0.013380239717662334,
       6.269964218139648,
       6.504695415496826],
      [7.026230812072754,
       0.009749860502779484,
       6.3509721755981445,
       6.799658298492432],
      [7.910851955413818,
       0.003932258579879999,
       7.14489221572876,
       7.45151948928833],
      [8.125563621520996,
       0.002840042347088456,
       6.81790828704834,
       7.730402946472168],
      [8.77963924407959,
       0.001877790899015963,
       7.331699848175049,
       8.173460960388184],
      [8.541739463806152,
       0.0026780301705002785,
       7.206151962280273,
       7.973222255706787],
      [8.880278587341309,
       0.001406369497999549,
       7.9754228591918945,
       8.48917293548584],
      [9.345046997070312,
       0.0007925212266854942,
       8.12267017364502,
       9.01477336883545],
      [9.43131160736084,
       0.0008042459958232939,
       8.057001113891602,
       9.183345794677734],
      [9.995135307312012,
       0.0006107916706241667,
       8.24326229095459,
       9.717299461364746],
      [10.098844528198242,
       0.0004766712663695216,
       8.232972145080566,
       9.732017517089844],
      [9.491938591003418,
       0.0014378979103639722,
       8.052389144897461,
       9.022835731506348],
      [9.62276554107666,
       0.0012428394984453917,
       8.101581573486328,
       9.154314994812012],
      [10.681485176086426,
       0.0004953498137183487,
       8.808384895324707,
       10.213825225830078],
      [10.046878814697266,
       0.001078583300113678,
       8.70515251159668,
       9.486727714538574],
      [10.256135940551758,
       0.0009287220309488475,
       8.431547164916992,
       9.683843612670898],
      [10.194869995117188,
       0.0012729403097182512,
       8.672965049743652,
       9.392139434814453],
      [10.591064453125,
       0.0005397357163019478,
       8.631051063537598,
       10.053897857666016],
      [11.356575965881348,
       0.00018060512957163155,
       9.033474922180176,
       10.988407135009766],
      [10.922317504882812,
       0.00039592283428646624,
       8.827499389648438,
       10.335823059082031],
      [11.239380836486816,
       0.00033430857001803815,
       9.127370834350586,
       10.612547874450684],
      [11.520257949829102,
       0.00022131245350465178,
       9.585162162780762,
       10.791105270385742],
      [11.521830558776855,
       0.00013479130575433373,
       9.359156608581543,
       11.092489242553711],
      [11.391655921936035,
       0.00013324664905667305,
       9.502625465393066,
       10.960362434387207],
      [11.424065589904785,
       0.00012796199007425457,
       9.544157028198242,
       10.991753578186035],
      [10.864642143249512,
       0.00029752420959994197,
       9.516603469848633,
       10.473531723022461],
      [11.666711807250977,
       0.00011878634541062638,
       10.205142974853516,
       11.155003547668457],
      [12.020890235900879,
       6.572023994522169e-05,
       10.499109268188477,
       11.61831283569336],
      [12.091333389282227,
       0.00010845055658137426,
       10.379804611206055,
       11.653849601745605],
      [12.579631805419922,
       6.31129732937552e-05,
       10.663679122924805,
       12.276350975036621],
      [12.243083000183105,
       5.188570139580406e-05,
       10.435660362243652,
       12.26773452758789],
      [11.816276550292969,
       0.00020267379295546561,
       10.240168571472168,
       11.445882797241211],
      [12.31605052947998,
       6.982527702348307e-05,
       10.178969383239746,
       12.382808685302734],
      [12.447364807128906,
       5.220527600613423e-05,
       10.530975341796875,
       12.532464027404785],
      [13.048710823059082,
       2.1387597371358424e-05,
       10.99190616607666,
       13.298393249511719],
      [12.873744010925293,
       2.921411578427069e-05,
       11.103266716003418,
       13.197416305541992],
      [12.91596794128418,
       2.6937874281429686e-05,
       11.179850578308105,
       13.236092567443848],
      [12.725324630737305,
       2.9441940569086e-05,
       11.069664001464844,
       12.98025894165039],
      [12.394413948059082,
       3.334244684083387e-05,
       10.640064239501953,
       12.920940399169922],
      [13.299117088317871,
       1.2729350601148326e-05,
       11.270634651184082,
       13.854862213134766],
      [12.81700611114502,
       1.8062073650071397e-05,
       10.843260765075684,
       13.29117488861084],
      [12.342144966125488,
       2.58339787251316e-05,
       10.880799293518066,
       12.568072319030762],
      [12.743487358093262,
       2.1828780518262647e-05,
       11.321049690246582,
       12.982192039489746],
      [12.513692855834961,
       2.5523078875266947e-05,
       11.076459884643555,
       12.765153884887695],
      [12.688060760498047,
       1.7876220226753503e-05,
       11.013333320617676,
       13.048667907714844],
      [13.115880966186523,
       1.6531099390704185e-05,
       11.255593299865723,
       13.473265647888184],
      [12.795315742492676,
       1.6668594980728813e-05,
       11.117300033569336,
       13.2176513671875],
      [12.719576835632324,
       1.8804177670972422e-05,
       11.205936431884766,
       13.075496673583984],
      [12.745713233947754,
       1.908206832013093e-05,
       11.202881813049316,
       13.090301513671875],
      [13.086715698242188,
       2.6389994673081674e-05,
       11.411826133728027,
       13.314336776733398],
      [13.047174453735352,
       2.056113771686796e-05,
       11.505082130432129,
       13.34873104095459],
      [13.435454368591309,
       1.3397397196968086e-05,
       11.625304222106934,
       13.846431732177734],
      [13.18504810333252,
       1.636051274545025e-05,
       11.559345245361328,
       13.569540023803711],
      [13.424245834350586,
       1.720351428957656e-05,
       11.433496475219727,
       13.871515274047852],
      [13.720685005187988,
       9.953289008990396e-06,
       11.709378242492676,
       14.482383728027344],
      [14.043994903564453,
       7.046943665045546e-06,
       12.073808670043945,
       14.861190795898438],
      [13.927314758300781,
       8.986617103801109e-06,
       11.782881736755371,
       14.846641540527344],
      [14.066973686218262,
       8.585553587181494e-06,
       11.782421112060547,
       14.881197929382324],
      [12.877187728881836,
       1.8178019672632217e-05,
       11.307437896728516,
       13.255533218383789],
      [13.187509536743164,
       1.6263677025563084e-05,
       11.29507064819336,
       13.70489501953125],
      [13.264122009277344,
       1.5606956367264502e-05,
       11.332417488098145,
       13.762785911560059],
      [13.415763854980469,
       1.2511343811638653e-05,
       11.758950233459473,
       13.848918914794922],
      [13.111907005310059,
       1.6938505723373964e-05,
       11.620128631591797,
       13.490671157836914],
      [13.654364585876465,
       6.553323146363255e-06,
       12.244131088256836,
       14.167896270751953],
      [13.803143501281738,
       6.7237297116662376e-06,
       12.07359790802002,
       14.444539070129395],
      [14.133207321166992,
       6.805551493016537e-06,
       12.366861343383789,
       14.761249542236328],
      [14.47835922241211,
       4.830975740333088e-06,
       12.457558631896973,
       15.478951454162598]],
     'step': 600}




```python
ts = '2019-12-29T000509Z'
with open(f'{workdir}/epoch_006_batch_00600_validation_losses.json') as fd:
    batch_losses_vec = json.load(fd)['batch_losses_vec']
    
lossesarr = np.array(batch_losses_vec)
meanlossesarr = np.mean(lossesarr, axis=1)

#batch_losses_vec[:5]
#batch_losses_vec = []
#for step in np.arange(0, 1068, 10):
# [2.8359528, 0.45356295, 1.7049086, 4.099845]

plt.plot([x[0] for x in batch_losses_vec], color='blue', label='0')
plt.plot([x[1] for x in batch_losses_vec], color='green', label='1')
plt.plot([x[2] for x in batch_losses_vec], color='red', label='2')
plt.plot([x[3] for x in batch_losses_vec], color='orange', label='3')
plt.plot(meanlossesarr, color='black', label='mean')
plt.title(f'validation losses  (model {ts}) after 6.5 epochs')
plt.legend()     
        
```




    <matplotlib.legend.Legend at 0x7f6e4a181eb8>




![png](2019-12-28-two-plot_files/2019-12-28-two-plot_9_1.png)

