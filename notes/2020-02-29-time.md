### 2020-02-29

#### Using this for measuring timing...
```

time python predict.py --test-loc history/2020-02-03T000055Z/test_balanced.h5 \
        --batch-size 32 \
        --model-loc history/2020-02-16T035758Z/epoch_001_batch_01760_model.h5  \
        --work-dir history/2020-02-29T2006Z \
        --eager

```

#### I tried a few different batch sizes and got few pretty cool results!
* So given the kaggle test set of 17.9million examples, that would have taken `25 hours` by my back of the envelope calculation , with the 56k examples/292 seconds
* And at the 56k examples/27s, thats more like `2.3hours` or so. 
```python
batchsize , seconds
32, 292
... # I tried 64, 256 too, but didn't write down the result.
1024, 27
2048, 26.8
```
* And I compared the loggloss outputs too and they were close to 3 digits of precision until `batch-size=1024` 
* So basically a clean win win here.

#### CPU usage and multiprocessing
* Only issue still was that monitoring the CPU usage, I would only see up to `100%` and not `200%`  , which is indeed odd even after I added the `multiprocessing` in commit `acaa4b`
* So with `4` processes to handle the input, they would just split the CPU as `25%`, `25%`, `25%`, `25%`, 


### 2020-03-07

#### approaching the full test set.
* need to first inspect the test data. Is it sequential or not at all? If not I basically cannot even use my algo on this.
* Got to split it into chunks, do my preprocessing: 
(1) _Make data:_ where I take a csv , extract my `8` features and window into my `64` units. And save pieces to `.h5` datasets.
(2) _Scaling:_ Apply my scalers (to all `8` features) and output to a new `blah-scaled.h5` dataset. 
(3) _Indexing and Overlapping Sequences:_ Somehow I got to maintain the indices without losing track, so the submission can be clean. The logical way of doing this is that  the corresponding label should be in the last row of the sequence. Might be a way to verify this. And of course the first `sixty three (64)` rows just dont get a prediction, or I can just give them random predictions.

#### Split test set by (crew,seat)
* In my notebook, _notebooks/aviation-pilot-physiology-hmm/2020-03-07-run-test-set.ipynb_ , I ended up using this bash approach below, since the data is indeed separate and using `awk`, with `sort` is nice since I don't need to keep everything in memory as with python.
```bash
(pandars3) $ for i in 1 2 3 4 5 6 7 8 13; 
   do 
   for j in 0 1 ; 
     do 
     outfile="data/test-crew-${i}_seat-${j}.csv"
     echo Starting ${outfile} $(date)

     awk -v crew=${i} -v seat=${j} \
              -F ',' 'BEGIN {OFS = "," } \
                      {if (($5 == seat || $5 == "seat") \
                          && ($2 == crew || $2 == "crew")) \
                      {print $0 } } \
                      ' data/test.csv \
         | sort -t ',' -k 2,2  --numeric-sort  >  ${outfile} 
     done 
   done

Starting data/test-crew-1_seat-0.csv Sun Mar 8 20:40:10 EDT 2020
Starting data/test-crew-1_seat-1.csv Sun Mar 8 20:46:13 EDT 2020
Starting data/test-crew-2_seat-0.csv Sun Mar 8 20:52:14 EDT 2020
Starting data/test-crew-2_seat-1.csv Sun Mar 8 20:58:29 EDT 2020
Starting data/test-crew-3_seat-0.csv Sun Mar 8 21:04:44 EDT 2020
Starting data/test-crew-3_seat-1.csv Sun Mar 8 21:10:49 EDT 2020
Starting data/test-crew-4_seat-0.csv Sun Mar 8 21:17:03 EDT 2020
Starting data/test-crew-4_seat-1.csv Sun Mar 8 21:23:10 EDT 2020
Starting data/test-crew-5_seat-0.csv Sun Mar 8 21:29:08 EDT 2020
Starting data/test-crew-5_seat-1.csv Sun Mar 8 21:35:24 EDT 2020
Starting data/test-crew-6_seat-0.csv Sun Mar 8 21:41:34 EDT 2020
Starting data/test-crew-6_seat-1.csv Sun Mar 8 21:47:38 EDT 2020
Starting data/test-crew-7_seat-0.csv Sun Mar 8 21:53:44 EDT 2020
Starting data/test-crew-7_seat-1.csv Sun Mar 8 21:59:46 EDT 2020
Starting data/test-crew-8_seat-0.csv Sun Mar 8 22:05:46 EDT 2020
Starting data/test-crew-8_seat-1.csv Sun Mar 8 22:11:49 EDT 2020
Starting data/test-crew-13_seat-0.csv Sun Mar 8 22:17:55 EDT 2020
Starting data/test-crew-13_seat-1.csv Sun Mar 8 22:23:49 EDT 2020

```
* So next can run make data on these and then predict. Each is only `250M` or so.
```
$ ls -larth data/test-crew-1*.csv
-rw-r--r--@ 1   staff   261M Mar  8 20:46 data/test-crew-1_seat-0.csv
-rw-r--r--@ 1   staff   275M Mar  8 20:52 data/test-crew-1_seat-1.csv
-rw-r--r--@ 1   staff   237M Mar  8 22:23 data/test-crew-13_seat-0.csv
-rw-r--r--@ 1   staff   238M Mar  8 22:29 data/test-crew-13_seat-1.csv
$ 
```
