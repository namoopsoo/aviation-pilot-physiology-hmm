
#### LSTM code seems to be shifting around slightly.
* So the audio example [here](https://www.svds.com/tensorflow-rnn-tutorial/) uses `tf.contrib.rnn.BasicLSTMCell`
* And per reading [docs](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn) that `BasicLSTMCell` is DEPRACATED, in favor of `tf.nn.rnn_cell.LSTMCell` 
* Then looking at the [LSTMCell docs](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMCell) , you're asked to import `tf.keras.layers.LSTMCell` instead.
* And as I'm looking through [Jeff Heaton's LSTM notebook](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class10_lstm.ipynb), 
he's clearly using `LSTM` instead, which apparently is [apparently](https://stackoverflow.com/questions/48187283/whats-the-difference-between-lstm-and-lstmcell#48187516) 
just a higher level abstraction of an `LSTMCell`. I think the abstraction sounds like a winner for now.

#### Quick try
* I want to first test-drive the cool Sun Spots Example in [Jeff Heaton's LSTM notebook](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class10_lstm.ipynb),
where Jeff Heaton predicts sun spot daily-count-sequences . 
* The slightly arbitrary `SEQUENCE_SIZE = 10` of how large the sequence window to feed into an LSTM is puzzling.
* Also this example appears to be predicting the *single* next sun spot number . Wow that's kind of cool how much flexibility one has here,
comparing the `Y` here to his earlier toy example of predicting a class.
* And there are a lot of parameters , including the number of LSTM cells (`units`), the `recurrent_dropout`, the `dropout`.
* The `input_shape=(None, 1)` in `model.add(LSTM(64, dropout=0.0, recurrent_dropout=0.0, input_shape=(None, 1)))`, 
that appears to be saying each sequence element has just one column. At least that's what I deduce here.
* Also in the toy example (which predicts the _color_ of a _"car"_ passing in front of a camera), 
he's using a model that binarizes the classes `0,1,2,3` as `4x1` vectors. I don't quite know why that helps.
```python
model = Sequential()
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, input_shape=(None, 1)))
model.add(Dense(4, activation='sigmoid'))
```
* But I did run the toy example with my own test data, and when printing the raw outputs, they are `4x1` probability looking things,
```python
xin = [
    [[2],[2],[0],[0],[0],[0]],
    [[0],[0],[0],[1],[1],[0]],
    [[0],[0],[0],[0],[3],[3]],
    [[0],[0],[0],[0],[0],[0]],
    [[0],[0],[0],[0],[2],[2]],
    [[3],[3],[0],[0],[0],[0]]
]
xin = np.array(xin, dtype=np.float32)
pred = model.predict(xin)
predict_classes = np.argmax(pred,axis=1)
print('raw pred ', pred)
print('sums, ', [f'{sum(foo):.2f}' for foo in pred])
print("Predicted classes: {}",predict_classes)
```
* But they're not probabilities since they sum to `['1.07', '1.03', '0.87', '1.07', '0.86', '1.02']`.
* So I would need to normalize that before sending that as a proba somewhere. 
```python
# =>
raw pred  [[1.0669231e-05 9.2744869e-01 1.4303035e-01 9.0453029e-04]
 [1.8352568e-03 6.7573059e-01 3.1061488e-01 3.7463784e-02]
 [1.3700724e-03 1.7304990e-01 1.3960657e-01 5.5343002e-01]
 [2.2188216e-02 7.3584348e-01 2.1823975e-01 9.5495105e-02]
 [3.1868815e-03 4.6828458e-01 1.6219813e-01 2.2529706e-01]
 [4.4703484e-07 3.1890899e-02 9.8183292e-01 2.2109449e-03]]
sums,  ['1.07', '1.03', '0.87', '1.07', '0.86', '1.02']
Predicted classes: {} [1 1 3 1 1 2]
```

### Appendix

#### Test run toy example
```python
# borrowing helpful code from Jeff Heaton
# source: https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class10_lstm.ipynb

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Embedding
from keras.layers import LSTM
from keras.datasets import imdb
import numpy as np

max_features = 4 # 0,1,2,3 (total of 4)
x = [
    [[0],[1],[1],[0],[0],[0]],
    [[0],[0],[0],[2],[2],[0]],
    [[0],[0],[0],[0],[3],[3]],
    [[0],[2],[2],[0],[0],[0]],
    [[0],[0],[3],[3],[0],[0]],
    [[0],[0],[0],[0],[1],[1]]
]
x = np.array(x,dtype=np.float32)
y = np.array([1,2,3,2,3,1],dtype=np.int32)

# Convert y2 to dummy variables
y2 = np.zeros((y.shape[0], max_features),dtype=np.float32)
y2[np.arange(y.shape[0]), y] = 1.0
print(y2)

print('Build model...')
model = Sequential()
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, input_shape=(None, 1)))
model.add(Dense(4, activation='sigmoid'))

# try using different optimizers and different optimizer configs
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

print('Train...')
model.fit(x, y2, epochs=200)
pred = model.predict(x)
predict_classes = np.argmax(pred, axis=1)
print("Predicted classes: {}", predict_classes)
print("Expected classes: {}", y)
```
```python
xin = [
    [[2],[2],[0],[0],[0],[0]],
    [[0],[0],[0],[1],[1],[0]],
    [[0],[0],[0],[0],[3],[3]],
    [[0],[0],[0],[0],[0],[0]],
    [[0],[0],[0],[0],[2],[2]],
    [[3],[3],[0],[0],[0],[0]]
]
xin = np.array(xin, dtype=np.float32)
pred = model.predict(xin)
predict_classes = np.argmax(pred,axis=1)
print('raw pred ', pred)
print('sums, ', [f'{sum(foo):.2f}' for foo in pred])
print("Predicted classes: {}",predict_classes)
```
```python
raw pred  [[1.0669231e-05 9.2744869e-01 1.4303035e-01 9.0453029e-04]
 [1.8352568e-03 6.7573059e-01 3.1061488e-01 3.7463784e-02]
 [1.3700724e-03 1.7304990e-01 1.3960657e-01 5.5343002e-01]
 [2.2188216e-02 7.3584348e-01 2.1823975e-01 9.5495105e-02]
 [3.1868815e-03 4.6828458e-01 1.6219813e-01 2.2529706e-01]
 [4.4703484e-07 3.1890899e-02 9.8183292e-01 2.2109449e-03]]
sums,  ['1.07', '1.03', '0.87', '1.07', '0.86', '1.02']
Predicted classes: {} [1 1 3 1 1 2]
```
