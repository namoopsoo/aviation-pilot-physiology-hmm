
#### LSTM code seems to be shifting around slightly.
* So the audio example [here](https://www.svds.com/tensorflow-rnn-tutorial/) uses `tf.contrib.rnn.BasicLSTMCell`
* And per reading [docs](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn) that `BasicLSTMCell` is DEPRACATED, in favor of `tf.nn.rnn_cell.LSTMCell` 
* Then looking at the [LSTMCell docs](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMCell) , you're asked to import `tf.keras.layers.LSTMCell` instead.
* And as I'm looking through [Jeff Heaton's LSTM notebook](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class10_lstm.ipynb), 
he's clearly using `LSTM` instead, which apparently is [apparently](https://stackoverflow.com/questions/48187283/whats-the-difference-between-lstm-and-lstmcell#48187516) 
just a higher level abstraction of an `LSTMCell`. I think the abstraction sounds like a winner for now.

#### Quick try
* I want to first test-drive the cool Sun Spots Example in [Jeff Heaton's LSTM notebook](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class10_lstm.ipynb),
where Jeff Heaton predicts sun spot daily-count-sequences . 
* The slightly arbitrary `SEQUENCE_SIZE = 10` of how large the sequence window to feed into an LSTM is puzzling.
* Also this example appears to be predicting the *single* next sun spot number . Wow that's kind of cool how much flexibility one has here,
comparing the `Y` here to his earlier toy example of predicting a class.
* And there are a lot of parameters , including the number of LSTM cells (`units`), the `recurrent_dropout`, the `dropout`.
* The `input_shape=(None, 1)` in `model.add(LSTM(64, dropout=0.0, recurrent_dropout=0.0, input_shape=(None, 1)))`, 
that appears to be saying each sequence element has just one column. At least that's what I deduce here.

