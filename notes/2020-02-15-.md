
#### Some next thoughts
* Hmm, per [this post](https://danijar.com/tips-for-training-recurrent-neural-networks/) , this idea of overlapping sequences is kind of an interesting idea. 
* And the _"Stacked recurrent networks"_ is also mentioned there, but the tip says to sum the outputs instead of I guess serializing them. Interesting. 
* As far as the _"Adaptive learning rate"_ (Adam) tip, this is part of the example tensorflow code, so I have been using that already.
* And as far as the advice to use _"Forget gate bias = 1"_ advice, well per just looking at the keras `LSTM` class,  the default of `unit_forget_bias=True` is already there. Heh.
* And as for the _recurrent dropout_ advice, I have been using that too, but maybe I have been going way too extreme. I have been using `dropout=0.5` and `recurrent_dropout=0.5` , but after fixing scaling issues now I want to try lowering this.
* And high level more data, more features, are also additional opportunities.
* And ensembling.

#### Dropout
* So indeed [next here](https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-02-15.md) , I tried reducing the dropout from `0.5` to `0.2` and wow much better than [here](https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-02-08-take2--update--2.md) . 
* Also saw through [Jason's post](https://machinelearningmastery.com/use-dropout-lstm-networks-time-series-forecasting/), I see the benefit of a nice systematic approach , comparing several dropout rates, `0, 0.2, 0.4, 0.6`. 
