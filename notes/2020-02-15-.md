
#### Some next thoughts
* Hmm, per [this post](https://danijar.com/tips-for-training-recurrent-neural-networks/) , this idea of overlapping sequences is kind of an interesting idea. 
* And the _"Stacked recurrent networks"_ is also mentioned there, but the tip says to sum the outputs instead of I guess serializing them. Interesting. 
* As far as the _"Adaptive learning rate"_ (Adam) tip, this is part of the example tensorflow code, so I have been using that already.
* And as far as the advice to use _"Forget gate bias = 1"_ advice, well per just looking at the keras `LSTM` class,  the default of `unit_forget_bias=True` is already there. Heh.
* And as for the _recurrent dropout_ advice, I have been using that too, but maybe I have been going way too extreme. I have been using `dropout=0.5` and `recurrent_dropout=0.5` , but after fixing scaling issues now I want to try lowering this.
* And high level more data, more features, are also additional opportunities.
* And ensembling.
