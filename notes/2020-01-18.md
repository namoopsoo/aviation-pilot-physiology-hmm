

```python

from importlib import reload
import os
import pandas as pd
from io import StringIO
import itertools
import ipdb
import datetime
from collections import Counter

import h5py
import json
import tensorflow as tf
from tensorflow import keras

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import joblib
print(tf.__version__)

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Embedding
from keras.layers import LSTM

from keras.callbacks import EarlyStopping

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
import mytf.s3utils as msu
import mytf.utils as mu
import mytf.validation as mv
import mytf.plot as mp
```

    1.14.0


    Using TensorFlow backend.



```python
tf.enable_eager_execution()

```

#### Mess around with the example weights of cross entropy
Borrow some of the mess code from [earlier book](https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-01-10-confidence--update.md#play-with-sparse_softmax_cross_entropy-a-little-here)


```python
preds = np.array([[ 1.83809385e-01,  1.00108096e-02, -1.75332371e-03,
         1.12673618e-01],
       [ 1.83426142e-01,  9.55305714e-03, -1.38717750e-03,
         1.12821095e-01],
       [ 1.83396950e-01,  9.47616715e-03, -1.38786808e-03,
         1.12689503e-01],])

for preds, ylabels, weights in [[np.array([[0, 0, 0, 0],
                                 [0, 0, 0, 0],
                                 [0, 0, 0, 0],
                                 [0, 0, 0, 0],
                                 [0, 0, 0, 0],]), np.array([0, 0, 0, 0, 0]),
                                None],
                      [np.array([[1, 0, 0, 0],
                                 [1, 0, 0, 0],
                                 [1, 0, 0, 0],
                                 [1, 0, 0, 0],
                                 [1, 0, 0, 0],]), np.array([0, 0, 0, 0, 0]),
                                  None],
                      [np.array([[1, 0, 0, 1],
                                 [1, 0, 0, 1],
                                 [1, 0, 0, 1],
                                 [1, 0, 0, 1],
                                 [1, 0, 0, 1],]), np.array([0, 0, 0, 0, 0]),
                                  None],
                      [np.array([[2, 0, 0, 0],
                                 [2, 0, 0, 0],
                                 [2, 0, 0, 0],
                                 [2, 0, 0, 0],
                                 [2, 0, 0, 0],]), np.array([0, 0, 0, 0, 0]),
                                  None],
                      [np.array([[9, 0, 0, 0],
                                 [9, 0, 0, 0],
                                 [9, 0, 0, 0],
                                 [9, 0, 0, 0],
                                 [9, 0, 0, 0],]), np.array([0, 0, 0, 0, 0]),
                                  None],
                      [np.array([[ 1.83809385e-01,  1.00108096e-02, -1.75332371e-03,
                         1.12673618e-01],
                       [ 1.83426142e-01,  9.55305714e-03, -1.38717750e-03,
                         1.12821095e-01],
                       [ 1.83396950e-01,  9.47616715e-03, -1.38786808e-03,
                         1.12689503e-01],]), np.array([0, 0, 0]),
                                  None],
                      [np.array([[9, 0, 0, 0],
                                 [9, 0, 0, 0],
                                 [9, 0, 0, 0],
                                 [9, 0, 0, 0],
                                 [9, 0, 0, 0],]), np.array([0, 0, 0, 0, 0]),
                                  np.array([1, 1, 1, 1, 1])],
                      [np.array([[9, 0, 0, 0],
                                 [9, 0, 0, 0]]), 
                                 np.array([0, 1]),
                                np.array([1, 0])],
                      [np.array([[9, 0, 0, 0],
                                 [9, 0, 0, 0]]), 
                                 np.array([0, 1]),
                                np.array([0, 1])]
                       
                      ]:
    kwargs = {**{'labels': ylabels.astype('int64'),
                 'logits': preds.astype('float64'),},
             **({'weights': weights} if weights is not None else {})}
    loss = tf.losses.sparse_softmax_cross_entropy(**kwargs).numpy()
    print({'ylabels': ylabels, 'preds': preds, 'weights': weights, 'loss': loss})
    print()
```

    {'ylabels': array([0, 0, 0, 0, 0]), 'preds': array([[0, 0, 0, 0],
           [0, 0, 0, 0],
           [0, 0, 0, 0],
           [0, 0, 0, 0],
           [0, 0, 0, 0]]), 'weights': None, 'loss': 1.3862943649291992}
    
    {'ylabels': array([0, 0, 0, 0, 0]), 'preds': array([[1, 0, 0, 0],
           [1, 0, 0, 0],
           [1, 0, 0, 0],
           [1, 0, 0, 0],
           [1, 0, 0, 0]]), 'weights': None, 'loss': 0.7436683773994446}
    
    {'ylabels': array([0, 0, 0, 0, 0]), 'preds': array([[1, 0, 0, 1],
           [1, 0, 0, 1],
           [1, 0, 0, 1],
           [1, 0, 0, 1],
           [1, 0, 0, 1]]), 'weights': None, 'loss': 1.0064088106155396}
    
    {'ylabels': array([0, 0, 0, 0, 0]), 'preds': array([[2, 0, 0, 0],
           [2, 0, 0, 0],
           [2, 0, 0, 0],
           [2, 0, 0, 0],
           [2, 0, 0, 0]]), 'weights': None, 'loss': 0.3407529592514038}
    
    {'ylabels': array([0, 0, 0, 0, 0]), 'preds': array([[9, 0, 0, 0],
           [9, 0, 0, 0],
           [9, 0, 0, 0],
           [9, 0, 0, 0],
           [9, 0, 0, 0]]), 'weights': None, 'loss': 0.00037016088026575744}
    
    {'ylabels': array([0, 0, 0]), 'preds': array([[ 0.18380938,  0.01001081, -0.00175332,  0.11267362],
           [ 0.18342614,  0.00955306, -0.00138718,  0.11282109],
           [ 0.18339695,  0.00947617, -0.00138787,  0.1126895 ]]), 'weights': None, 'loss': 1.281795859336853}
    
    WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.where in 2.0, which has the same broadcast rule as np.where
    {'ylabels': array([0, 0, 0, 0, 0]), 'preds': array([[9, 0, 0, 0],
           [9, 0, 0, 0],
           [9, 0, 0, 0],
           [9, 0, 0, 0],
           [9, 0, 0, 0]]), 'weights': array([1, 1, 1, 1, 1]), 'loss': 0.00037016088026575744}
    
    {'ylabels': array([0, 1]), 'preds': array([[9, 0, 0, 0],
           [9, 0, 0, 0]]), 'weights': array([1, 0]), 'loss': 0.00037016088026575744}
    
    {'ylabels': array([0, 1]), 'preds': array([[9, 0, 0, 0],
           [9, 0, 0, 0]]), 'weights': array([0, 1]), 'loss': 9.000370025634766}
    


Ok then I suppose for sake of argument, since the `label=1` loss is the only one getting 
minimized, I want to set that weight to always a low weight and just see what happens...


```python
# Working dir... for new model
save_dir = 'history'
ts = mu.quickts(); print('starting,', ts)

workdir = f'{save_dir}/{ts}'
os.mkdir(workdir)
print(f'Made new workdir, {workdir}')
```

    starting, 2020-01-18T200155Z
    Made new workdir, history/2020-01-18T200155Z



```python
lstm_params = [{
    'units': 64,
    'dropout': 0.5,
    'recurrent_dropout': 0.5,
    'batch_input_shape': (None, 64, 8),
    'kernel_initializer': tf.initializers.glorot_normal() # GlorotNormal()
                           #tf.initializers.he_normal()
    },

]

optimizer_params = {
    'learning_rate': 0.001,  
    'beta1': 0.9, 
    'beta2': 0.999, 
    'epsilon': 1e-08
}

def bake_model(lstm_params):

    model = tf.keras.Sequential([
        tf.keras.layers.LSTM(**lstm_params[0]),
        # 4 because 'A', 'B', 'C', 'D'.
        tf.keras.layers.Dense(4)])
    return model
```


```python

BATCH_SIZE = 32
EPOCHS = 1
# Use datasets from 
# 2019-12-25.ipynb
# 
# train ... new datasets, ...
datadir = 'history/2019-12-22T174803Z'
# train_loc = f'{datadir}/train_balanced.h5'
test_loc = f'{datadir}/test_balanced.h5'
train_shuff_loc = f'{datadir}/train_scaled_balanced_shuffled.h5'
print(mu.h5_keys(train_shuff_loc))
print(mu.h5_keys(test_loc))

X, Ylabels = mu.read_h5_two(
                source_location=train_shuff_loc, 
                Xdataset=f'X',
                Ydataset=f'Ylabels')
size = X.shape[0]

# save base unfitted model.
model = bake_model(lstm_params)
mu.save_model(model=model, 
              loc=f'{workdir}/00000__unfitted_model.h5')


```

    ['X', 'Ylabels']
    ['X_0', 'X_1', 'X_2', 'X_3', 'Ylabels_0', 'Ylabels_1', 'Ylabels_2', 'Ylabels_3']



```python
%%time
modelloc = f'{workdir}/00000__unfitted_model.h5'
print(f'Start train with {modelloc}')
model = mu.load_model(modelloc)

class_weights = {0: 1., 1: 0., 2: 0., 3: 0.}
dataset_batches = mu.build_dataset_weighty_v3(
        {'x_train': X,
         'ylabels_train': Ylabels.astype('int64')},
        list(range(size)), 
        class_weights,
        batch_size=BATCH_SIZE)
    
mu.do_train( 
        model,
        dataset_batches,
        k=size,
        epochs=EPOCHS,
        optimizer_params=optimizer_params,
        saveloc=workdir)
```

    Start train with history/2020-01-18T200155Z/00000__unfitted_model.h5
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
    
    
    0it [00:00, ?it/s][A[A[A
    ...
    1094it [07:05,  2.57it/s][A[A[A

    CPU times: user 7min 3s, sys: 1.04 s, total: 7min 4s
    Wall time: 7min 5s


    

#### checkpoint hmm

```python
# Look at a most recent train loss plot
historydir = 'history'
with open(f'{workdir}/epoch_000_batch_01090_train_loss_history.json') as fd:
    losshistory = json.load(fd)
    
plt.plot(losshistory) 
plt.title('Train xentropy logloss on epoch=0,batch=1090')
```




    Text(0.5, 1.0, 'Train xentropy logloss on epoch=0,batch=1090')




![png](2020-01-18_files/2020-01-18_9_1.png)



```python
epoch = 0
print(len(list(np.arange(0, 1100, 200))))
for batch in tqdm(list(np.arange(0, 1100, 200))):
    step = batch
    prefix = (f'{workdir}/epoch_{str(epoch).zfill(3)}'
                           f'_batch_{str(batch).zfill(5)}')

    modelname = f'{prefix}_model.h5'
    print(modelname, os.path.exists(modelname))
```

    6



    HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))


    history/2020-01-18T200155Z/epoch_000_batch_00000_model.h5 True
    history/2020-01-18T200155Z/epoch_000_batch_00200_model.h5 True
    history/2020-01-18T200155Z/epoch_000_batch_00400_model.h5 True
    history/2020-01-18T200155Z/epoch_000_batch_00600_model.h5 True
    history/2020-01-18T200155Z/epoch_000_batch_00800_model.h5 True
    history/2020-01-18T200155Z/epoch_000_batch_01000_model.h5 True
    



```python
print('starting validation', mu.quickts())
batch_losses_vec = []

epoch = 0
for batch in tqdm(list(np.arange(0, 1100, 200))):
    step = batch
    prefix = (f'{workdir}/epoch_{str(epoch).zfill(3)}'
                           f'_batch_{str(batch).zfill(5)}')

    modelname = f'{prefix}_model.h5'
    print(modelname, os.path.exists(modelname))

    steploss = mv.perf_wrapper(modelname,
                               dataloc=test_loc,
                               eager=True,
                              batch_size=32)
    batch_losses_vec.append([float(x) for x in steploss])
    mv.json_save({'batch_losses_vec': batch_losses_vec,
                  'step': int(step)
              }, 
              f'{prefix}_validation_losses.json')
    
print('done validation', mu.quickts())
#####
lossesarr = np.array(batch_losses_vec)
meanlossesarr = np.mean(lossesarr, axis=1)

batch_losses_vec[:5]
#batch_losses_vec = []
#for step in np.arange(0, 1068, 10):
# [2.8359528, 0.45356295, 1.7049086, 4.099845]

plt.plot([x[0] for x in batch_losses_vec], color='blue', label='0')
plt.plot([x[1] for x in batch_losses_vec], color='green', label='1')
plt.plot([x[2] for x in batch_losses_vec], color='red', label='2')
plt.plot([x[3] for x in batch_losses_vec], color='orange', label='3')
plt.plot(meanlossesarr, color='black', label='mean')
plt.title(f'validation losses  (model {ts})')
plt.legend()     
        
```

    starting validation 2020-01-18T204828Z



    HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))


    history/2020-01-18T200155Z/epoch_000_batch_00000_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.
    history/2020-01-18T200155Z/epoch_000_batch_00200_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.
    history/2020-01-18T200155Z/epoch_000_batch_00400_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.
    history/2020-01-18T200155Z/epoch_000_batch_00600_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.
    history/2020-01-18T200155Z/epoch_000_batch_00800_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.
    history/2020-01-18T200155Z/epoch_000_batch_01000_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.
    
    done validation 2020-01-18T212308Z





    <matplotlib.legend.Legend at 0x7f669cb35358>




![png](2020-01-18_files/2020-01-18_11_4.png)


Nice ok so as hoped, I can control label=0 to be learned instead.

Next what if I can dynamically control this. So, for each batch, 
use the batch losses to calibrate the weights for the next batch somehow.
