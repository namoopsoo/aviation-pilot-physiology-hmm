{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from importlib import reload\n",
    "import os\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import itertools\n",
    "import ipdb\n",
    "import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import h5py\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import mytf.s3utils as msu\n",
    "import mytf.utils as mu\n",
    "import mytf.plot as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_loss(loss_history, wheredir):\n",
    "    ts = mu.quickts()\n",
    "    with open(f'{wheredir}/{ts}.json', 'w') as fd:\n",
    "        json.dump(loss_history, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34205\n"
     ]
    }
   ],
   "source": [
    "# continue from 2019-12-01 notebook where I had a MemoryError, \n",
    "\n",
    "# \n",
    "# Grab and randomize since it's not huge...\n",
    "vecs = [mu.read_h5_two(source_location='data/2019-12-07-train-balanced.h5', \n",
    "                    Xdataset=f'X_{i}',\n",
    "                    Ydataset=f'Ylabels_{i}')\n",
    "                 for i in [0, 1, 2, 3]]\n",
    "\n",
    "X_train = np.concatenate([x[0] for x in vecs])\n",
    "Ylabels_train = np.concatenate([x[1] for x in vecs])\n",
    "\n",
    "# shuffle...\n",
    "size = X_train.shape[0]\n",
    "print(size)\n",
    "indices = np.random.choice(range(size), size=size, replace=False)\n",
    "X_train_shfl = X_train[indices]\n",
    "Ylabels_train_shfl = Ylabels_train[indices].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34205, 64, 8), (34205,), (34205, 64, 8), (34205,))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Ylabels_train.shape, X_train_shfl.shape, Ylabels_train_shfl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0.0: 9455, 1.0: 7625, 2.0: 8286, 3.0: 8839}),\n",
       " Counter({2: 8286, 3: 8839, 0: 9455, 1: 7625}))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(Ylabels_train), Counter(Ylabels_train_shfl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1650.    0. 8286.    0.], shape=(4,), dtype=float64)\n",
      "{0: 1650, 2: 8286}\n",
      "[1650, 0, 8286, 0]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "with h5py.File('data/train.h5', 'r+') as fd:\n",
    "#    print(list(fd.keys()))\n",
    "\n",
    "\n",
    "#    X = fd[f'X_{i}'].__array__()\n",
    "    Y = fd['dataset_0_Y'].__array__()\n",
    "class_counts = tf.reduce_sum(Y, axis=0)\n",
    "labels = np.argmax(Y, axis=1)\n",
    "\n",
    "print(class_counts)\n",
    "print(dict(Counter(labels)))\n",
    "adict = dict(Counter(labels))\n",
    "print([adict.get(i, 0) for i in [0, 1, 2, 3]])\n",
    "#print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mytf.utils' from '/home/ec2-user/SageMaker/aviation-pilot-physiology-hmm/mytf/utils.py'>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 21:54:40.448859 140283925907264 hdf5_format.py:221] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "m = mu.load_model('history/2019-12-08T215137Z/00000_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=75755, shape=(3, 4), dtype=float32, numpy=\n",
       "array([[ 0.27179655, -0.28642282,  0.22414565,  0.45910197],\n",
       "       [-0.17425132, -0.03353608,  0.07828245,  0.05538325],\n",
       "       [ 0.22939533, -0.1261261 ,  0.32558665,  0.4667279 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(tf.convert_to_tensor(X, dtype=tf.float32)[:3])\n",
    "#type(tf.convert_to_tensor(X)[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1069it [00:00, 4570.92it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch, blah in tqdm(enumerate(dataset_batches.take(size))):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size//32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting, 2019-12-08T220612Z\n",
      "num slices 1068\n",
      "size_remainder,  29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/aviation-pilot-physiology-hmm/mytf/utils.py:185: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  )/class_counts\n",
      "1069it [07:28,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 24s, sys: 761 ms, total: 7min 25s\n",
      "Wall time: 7min 28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "loss_history = []\n",
    "validation_loss_history = []\n",
    "save_dir = 'history'\n",
    "BATCH_SIZE = 32\n",
    "ts = mu.quickts(); print('starting,', ts)\n",
    "X = X_train_shfl\n",
    "Ylabels = Ylabels_train_shfl\n",
    "size = X_train_shfl.shape[0]\n",
    "workdir = f'{save_dir}/{ts}'\n",
    "os.mkdir(workdir)\n",
    "#Counter(Ylabels_train), Counter(Ylabels_train_shfl)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(64,   dropout=0.2, recurrent_dropout=0.2,\n",
    "                batch_input_shape=(None, 64, 8), \n",
    "              ),\n",
    "    # 4 because 'A', 'B', 'C', 'D'.\n",
    "    tf.keras.layers.Dense(4)])\n",
    "\n",
    "class_weights = {0: 1., 1: 1., 2: 1., 3: 1.}\n",
    "# parts = get_partitions(range(size), slice_size=BATCH_SIZE)\n",
    "# training_indices = np.arange(0, X.shape[0], 1)\n",
    "dataset_batches = mu.build_dataset_weighty_v3(\n",
    "        {'x_train': X,\n",
    "         'ylabels_train': Ylabels},  # 'ylabels_train'  if i have labels\n",
    "        list(range(size)), \n",
    "        class_weights,\n",
    "        batch_size=BATCH_SIZE)\n",
    "    \n",
    "with ipdb.launch_ipdb_on_exception():\n",
    "    mu.do_train(\n",
    "        model,\n",
    "        dataset_batches,\n",
    "        k=size,\n",
    "        saveloc=workdir)\n",
    "#    loss_history_this = [float(x) for x in loss_history_this]\n",
    "#    loss_history.extend(loss_history_this)\n",
    "#    #save_loss(loss_history, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
