
#### Problems with yesterday
* Whether I should have been using `loss='binary_crossentropy'` or weighted, I think is less consequential,
than the fact that the output layer I had did not output a probability looking thing, meaning the outputs 
didn't sum to 1. 
* So maybe that was a problem. So this is where _softmax_ I have read can help.
* I was reading about [weighted_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits), 
but I dont think this does what I hoped for. I have imbalanced classes `A, C`, occuring more frequently than `B, D` , 
so I care about increasing the  weight of the *cost* of errors on the rare classes. 
* Okay [this](https://datascience.stackexchange.com/a/15194) answer for [q](https://datascience.stackexchange.com/questions/12886/tensorflow-adjusting-cost-function-for-imbalanced-data) seems to do that, manually defining a cost function.
* So there are many losses defined [here](https://keras.io/losses/), there is in particular `"categorical_crossentropy"` , but looking at  [the codes](https://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py#L3527),
it doesn't look like there is a handling of the balance of the categories /classes.

#### Re-ran first using `"categorical_crossentropy"`
* _(Ran this on the SageMaker notebook)_ : [model-4](model-4-func) 
* But looking at the confusion matrix, this didn't really help with the unbalanced classes at all.
```python
# ok ran model-4 on Sagemaker notebook w/ one change, making use of the categorical 
restored_model4 = keras.models.load_model('models/2019-05-19T001217-UTC-model-4.h5')

# do preds... get confusion..
outdata = make_data(df, crews={'training': [1],
                        'test': [2]},
              sequence_window=256, percent_of_data=1,
             feature_cols={'r': simple_scaler})
             
%time preds, preds_classes = do_get_preds(outdata, restored_model4)
# Wall time: 21min 10s

confusion_yea = do_confusiony(outdata, preds_classes)

# ==> wow this didnt help.
array([[325198,      0,      0,      0],
       [ 15371,      0,      0,      0],
       [183718,      0,      0,      0],
       [ 27039,      0,      0,      0]], dtype=int32)


```

#### Custom loss func
* Started experimenting [here](https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-05-18-book.ipynb)

### Ref
* [more , on loss funcs](https://towardsdatascience.com/custom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a)

### Appendix

#### model 4 func
```python
def bake_model4_may18(x_train, y_train, x_test, y_test, epochs=1, **kwargs):
    
    model = Sequential()
    model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2,
                   input_shape=(None, 1)))

    # 4 because 'A', 'B', 'C', 'D'.
    model.add(Dense(4))

    model.compile(loss='categorical_crossentropy', # unbalanced_categorical_crossentropy,
                  optimizer='adam',
                  metrics=['accuracy'])
    
    # ðŸ¤” model.compile(loss='categorical_crossentropy', optimizer='adam')
    # initial_state ... for the LSTM , hmm

    monitor = EarlyStopping(monitor='val_loss',
                            min_delta=1e-3, patience=5, verbose=1, mode='auto')
    print('Train...', timestamp())

    # https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#arguments_8
    # - hmm so fit() can take a generator sometimes.
    # - use_multiprocessing=True 
    model.fit(x_train, y_train, validation_data=(x_test, y_test),
              callbacks=[monitor], verbose=2, epochs=epochs)

    return model
    
def runner_model4(df):
    print('Start make_data', timestamp())
    outdata = make_data(df, crews={'training': [1],
                        'test': [2]},
              sequence_window=256, percent_of_data=1,
             feature_cols={'r': simple_scaler})
    
    validate_data(outdata)

    print('Start bake_model', timestamp())
    model = bake_model4_may18(**outdata, epochs=2)
    return outdata, model
```

#### Additional validation
```python
def do_get_preds(outdata, model):
    preds = model.predict(outdata['x_test'])
    preds_classes = np.argmax(preds, axis=1)
    # print('Counter predict classes', Counter(preds_classes))
    # print('original', Counter(outdata['y_test_original']))
    # 
    return preds, preds_classes
    # 
    # 
    # preds, preds_classes = do_get_preds_and_confusion(outdata, model)
    
def do_confusiony(outdata, preds_classes):
    tensor = tf.confusion_matrix( 
    encode_class(outdata['y_test_original']),# labels
    preds_classes, # predictions
    num_classes=4)

    # Construct a `Session` to execute the graph.
    with tf.Session() as sess:

        # Execute the graph and store the value that `e` represents in `result`.
        result = sess.run(tensor)
        # print(result)
        return result
    
   
```

#### Draft custom loss func
```python

# forking from here:
# https://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py#L3527
def unbalanced_categorical_crossentropy(target, output, from_logits=False, axis=-1):
    """Categorical crossentropy between an output tensor and a target tensor.
    # Arguments
        target: A tensor of the same shape as `output`.
        output: A tensor resulting from a softmax
            (unless `from_logits` is True, in which
            case `output` is expected to be the logits).
        from_logits: Boolean, whether `output` is the
            result of a softmax, or is a tensor of logits.
        axis: Int specifying the channels axis. `axis=-1`
            corresponds to data format `channels_last`,
            and `axis=1` corresponds to data format
            `channels_first`.
    # Returns
        Output tensor.
    # Raises
        ValueError: if `axis` is neither -1 nor one of
            the axes of `output`.
    """
    output_dimensions = list(range(len(output.get_shape())))
    if axis != -1 and axis not in output_dimensions:
        raise ValueError(
            '{}{}{}'.format(
                'Unexpected channels axis {}. '.format(axis),
                'Expected to be -1 or one of the axes of `output`, ',
                'which has {} dimensions.'.format(len(output.get_shape()))))
        
    # Pre-computed Hard-coded class_weights...
    class_weights = {0: 0.5898470233582309,
                     1: 0.027880056445732652,
                     2: 0.33322934162364914,
                     3: 0.0490435785723873}
    # Note: tf.nn.softmax_cross_entropy_with_logits
    # expects logits, Keras expects probabilities.
    if not from_logits:
        # scale preds so that the class probas of each sample sum to 1
        output /= tf.reduce_sum(output, axis, True)
        # manual computation of crossentropy
        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)
        output = tf.clip_by_value(output, _epsilon, 1. - _epsilon)
        return - tf.reduce_sum(target * tf.log(output), axis)
    else:
        error = tf.nn.softmax_cross_entropy_with_logits(labels=target,
                                                       logits=output)
    
        # Scale by class
        class_weight = 
        return  tf.mul(error, class_weight)

```
