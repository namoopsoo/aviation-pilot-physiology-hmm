
#### Problems with yesterday
* Whether I should have been using `loss='binary_crossentropy'` or weighted, I think is less consequential,
than the fact that the output layer I had did not output a probability looking thing, meaning the outputs 
didn't sum to 1. 
* So maybe that was a problem. So this is where _softmax_ I have read can help.
* I was reading about [weighted_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits), 
but I dont think this does what I hoped for. I have imbalanced classes `A, C`, occuring more frequently than `B, D` , 
so I care about increasing the  weight of the *cost* of errors on the rare classes. 
* Okay [this](https://datascience.stackexchange.com/a/15194) answer for [q](https://datascience.stackexchange.com/questions/12886/tensorflow-adjusting-cost-function-for-imbalanced-data) seems to do that, manually defining a cost function.


