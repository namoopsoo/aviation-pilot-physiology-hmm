{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import os\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import itertools\n",
    "import ipdb\n",
    "import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import h5py\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "print(tf.__version__)\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.compat.v1.losses import sparse_softmax_cross_entropy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import mytf.s3utils as msu\n",
    "import mytf.utils as mu\n",
    "import mytf.validation as mv\n",
    "import mytf.plot as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting, 2020-02-09T010715Z\n",
      "Made new workdir, history/2020-02-09T010715Z\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir = 'history/2020-02-03T000055Z'\n",
    "ts = mu.quickts(); print('starting,', ts)\n",
    "\n",
    "workdir = f'history/{ts}'\n",
    "os.mkdir(workdir)\n",
    "print(f'Made new workdir, {workdir}')\n",
    "os.listdir(workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_params = [{\n",
    "    'units': 64,\n",
    "    'dropout': 0.5,\n",
    "    'recurrent_dropout': 0.5,\n",
    "    'batch_input_shape': (None, 64, 8),\n",
    "    'kernel_initializer': tf.initializers.glorot_normal() # GlorotNormal()\n",
    "                           #tf.initializers.he_normal()\n",
    "    },]\n",
    "\n",
    "optimizer_params = {\n",
    "    'learning_rate': 0.001,  \n",
    "    'beta1': 0.9, \n",
    "    'beta2': 0.999, \n",
    "    'epsilon': 1e-08\n",
    "}\n",
    "\n",
    "def bake_model(lstm_params):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(**lstm_params[0]),\n",
    "        # 4 because 'A', 'B', 'C', 'D'.\n",
    "        tf.keras.layers.Dense(4)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datadir history/2020-02-03T000055Z workdir history/2020-02-09T010715Z\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 4\n",
    "#\n",
    "print('datadir', datadir, 'workdir', workdir)\n",
    "train_shuff_loc = f'{datadir}/train_scaled_balanced_shuffled.h5'\n",
    "\n",
    "X, Ylabels = mu.read_h5_two(\n",
    "                source_location=train_shuff_loc, \n",
    "                Xdataset=f'X',\n",
    "                Ydataset=f'Ylabels')\n",
    "size = X.shape[0]\n",
    "\n",
    "# save base unfitted model.\n",
    "model = bake_model(lstm_params)\n",
    "mu.save_model(model=model, \n",
    "              loc=f'{workdir}/00000__unfitted_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start train with history/2020-02-09T010715Z/00000__unfitted_model.h5\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14e205d11414989adcee28fed3854ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84810d1a066a4cd7bee5ca3d0188d0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad747bf2a34848cab34d88e58ad0d716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab11dc353484869a95da5f384e7910d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 46min 33s, sys: 3.1 s, total: 46min 36s\n",
      "Wall time: 46min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelloc = f'{workdir}/00000__unfitted_model.h5'\n",
    "print(f'Start train with {modelloc}')\n",
    "model = mu.load_model(modelloc)\n",
    "\n",
    "class_weights = {0: 1., 1: 0., 2: 0., 3: 0.}\n",
    "dataset_batches = mu.build_dataset_weighty_v3(\n",
    "        {'x_train': X,\n",
    "         'ylabels_train': Ylabels.astype('int64')},\n",
    "        list(range(size)), \n",
    "        class_weights,\n",
    "        batch_size=BATCH_SIZE)\n",
    "    \n",
    "mu.do_train(\n",
    "        model,\n",
    "        dataset_batches,\n",
    "        k=size,\n",
    "        num_epochs=EPOCHS,\n",
    "        optimizer_params=optimizer_params,\n",
    "        saveloc=workdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loc ['X_0', 'X_1', 'X_2', 'X_3', 'Ylabels_0', 'Ylabels_1', 'Ylabels_2', 'Ylabels_3']\n"
     ]
    }
   ],
   "source": [
    "test_loc = f'{datadir}/test_balanced.h5'\n",
    "print('test_loc', mu.h5_keys(test_loc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelnames_vec = []\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in list(np.arange(200, 1760, 200)) + [1760]:\n",
    "        step = batch\n",
    "        prefix = (f'{workdir}/epoch_{str(epoch).zfill(3)}'\n",
    "                               f'_batch_{str(batch).zfill(5)}')\n",
    "\n",
    "        modelname = f'{prefix}_model.h5'\n",
    "        print(modelname, os.path.exists(modelname))\n",
    "        modelnames_vec.append(prefix)\n",
    "print('modelnames_vec', modelnames_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('starting validation', mu.quickts())\n",
    "batch_losses_vec = []\n",
    "print('test_loc', test_loc)\n",
    "epoch = 0\n",
    "for step, prefix in enumerate(tqdm(modelnames_vec)):\n",
    "    # prefix = (f'{workdir}/epoch_{str(epoch).zfill(3)}'\n",
    "    #                        f'_batch_{str(batch).zfill(5)}')\n",
    "\n",
    "    modelname = f'{prefix}_model.h5'\n",
    "    print(modelname, os.path.exists(modelname))\n",
    "\n",
    "    steploss = mv.perf_wrapper(modelname,\n",
    "                               dataloc=test_loc,\n",
    "                               eager=True,\n",
    "                              batch_size=32)\n",
    "    batch_losses_vec.append([float(x) for x in steploss])\n",
    "    mu.to_json_local({'batch_losses_vec': batch_losses_vec,\n",
    "                  'step': int(step)\n",
    "              }, \n",
    "              f'{prefix}_validation_losses.json')\n",
    "    \n",
    "print('done validation', mu.quickts())\n",
    "#####\n",
    "lossesarr = np.array(batch_losses_vec)\n",
    "meanlossesarr = np.mean(lossesarr, axis=1)\n",
    "\n",
    "batch_losses_vec[:5]\n",
    "#batch_losses_vec = []\n",
    "\n",
    "plt.plot([x[0] for x in batch_losses_vec], color='blue', label='0')\n",
    "plt.plot([x[1] for x in batch_losses_vec], color='green', label='1')\n",
    "plt.plot([x[2] for x in batch_losses_vec], color='red', label='2')\n",
    "plt.plot([x[3] for x in batch_losses_vec], color='orange', label='3')\n",
    "plt.plot(meanlossesarr, color='black', label='mean')\n",
    "plt.title(f'validation losses  (model {ts})')\n",
    "plt.legend()     \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
