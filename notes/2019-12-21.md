



Going to do another make_data() 
But first, take out the scaling part so I can do MinMax instead , later on.




```python

from importlib import reload
import os
import pandas as pd
from io import StringIO
import itertools
import ipdb
import datetime
from collections import Counter

import h5py
import json
import tensorflow as tf
from tensorflow import keras

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

print(tf.__version__)

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Embedding
from keras.layers import LSTM

from keras.callbacks import EarlyStopping

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
import mytf.s3utils as msu
import mytf.utils as mu
import mytf.plot as mp
```

    1.14.0


    Using TensorFlow backend.



```python
tf.enable_eager_execution()
```


```python
%%time

# I had 
crew1df = pd.read_pickle('data/crew_1-train.pkl')
crew2df = pd.read_pickle('data/crew_2-train.pkl')

df1to2 = pd.concat([crew1df, crew2df])

cols = ['r', 'ecg', 'gsr', 
        'eeg_fp1','eeg_f7', 'eeg_f8', 'eeg_t4', 'eeg_t6', ]

ts = mu.quickts()
newdir = f'data/{ts}'
os.mkdir(newdir)
print(f'Writing to {newdir} , just created.')

# with ipdb.launch_ipdb_on_exception():
out = mu.make_data(df1to2, crews={'training': [1],
                    'test': [2]},
              window_size=64,
               row_batch_size=10000,
         feature_cols=cols,
              save_dir=newdir)

```

    Writing to data/2019-12-21T215926Z , just created.
    Start building training set 2019-12-21T215926Z
    num slices 44
    size_remainder,  7652
    Start building testing set 2019-12-21T220000Z
    num slices 55
    size_remainder,  2868
    CPU times: user 1min 8s, sys: 8.94 s, total: 1min 17s
    Wall time: 1min 18s



```python
# There is a lot a lot of data, split up into multiple slices, 
# so to do the MinMaxScaler , can I just grab biggest from each, 
# and create a scaler from that and then just transform live later? 
#
# or more generally can you .fit() more than once 
# and what does that imply?
```


```python
from sklearn.preprocessing import MinMaxScaler
X = np.array([[-1, 2], [-0.5, 6], [0, 10], [1, 18]])
scaler = MinMaxScaler()
print(scaler.fit(X))

print(scaler.data_min_, scaler.data_max_)

print(scaler.transform(X))
# X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))

```

    MinMaxScaler(copy=True, feature_range=(0, 1))
    [-1.  2.] [ 1. 18.]
    [[0.   0.  ]
     [0.25 0.25]
     [0.5  0.5 ]
     [1.   1.  ]]



```python
X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
print(X_std)
```

    [[0.   0.  ]
     [0.25 0.25]
     [0.5  0.5 ]
     [1.   1.  ]]


#### Oh wow partial fit? 
Signature: scaler.partial_fit(X, y=None)
Docstring:
Online computation of min and max on X for later scaling.
All of X is processed as a single batch. This is intended for cases
when `fit` is not feasible due to very large number of `n_samples`
or because X is read from a continuous stream.

```
scaler = MinMaxScaler()
#scaler.set_params
# scaler.partial_fit?
# 
```



```python
# Nice so based on this... partial_fit of parts is equivalent
# looking to a full fit
X = np.array([[-1, 2], [-0.5, 6], [0, 10], [1, 18]])

scaler = MinMaxScaler()
for i in range(4):
    print(f'i: {i}')
    scaler.partial_fit(X[i:i+1,:])

    print(scaler.data_min_, scaler.data_max_)

    print(scaler.transform(X))

```

    i: 0
    [-1.  2.] [-1.  2.]
    [[ 0.   0. ]
     [ 0.5  4. ]
     [ 1.   8. ]
     [ 2.  16. ]]
    i: 1
    [-1.  2.] [-0.5  6. ]
    [[0. 0.]
     [1. 1.]
     [2. 2.]
     [4. 4.]]
    i: 2
    [-1.  2.] [ 0. 10.]
    [[0.  0. ]
     [0.5 0.5]
     [1.  1. ]
     [2.  2. ]]
    i: 3
    [-1.  2.] [ 1. 18.]
    [[0.   0.  ]
     [0.25 0.25]
     [0.5  0.5 ]
     [1.   1.  ]]



```python
def read_h5_raw(source_location, name):
    with h5py.File(source_location, 'r+') as fd:
        return fd[name].__array__()

def h5_keys(loc):
    with h5py.File(loc, 'r+') as fd:
        return list(fd.keys())

```


```python
# Ah cool, so indeed zip() is its own inverse.
# a = np.array(params_vec)
# b = np.array(list(zip(*params_vec)))
# print(a)
# print(np.array(list(zip(*b))))
```

    [[662.062012 666.276001]
     [654.408997 676.028015]
     [653.052979 676.028015]
     [653.052979 676.028015]
     [650.200989 676.028015]]
    [[662.062012 666.276001]
     [654.408997 676.028015]
     [653.052979 676.028015]
     [653.052979 676.028015]
     [650.200989 676.028015]]



```python
# Next:
# Oh wow that's perfect ! ... So I can use this to first create a 
# Standard scaler, across the whole dataset i have. perfect.

def build_scaler_from_h5(loc, datasets, feature):
    # one label at a time here..
    scaler = MinMaxScaler()
    params_vec = []
    for name in tqdm(datasets):
        X = read_h5_raw(loc, name)
        fullsize = X.shape[0]*X.shape[1]
        scaler.partial_fit(
            np.reshape(X[:, :, feature].ravel(),
                       (fullsize, 1)))
        params_vec.append(
            [scaler.data_min_[0], scaler.data_max_[0]])

    return scaler, params_vec
```


```python
trainloc = 'data/2019-12-21T215926Z/train.h5'

first = [x for x in h5_keys(trainloc) 
                               if '_X' in x][0]
X = read_h5_raw(trainloc, first)
scaler = MinMaxScaler()
scaler.partial_fit(np.reshape(X[:, :, 0].ravel(), (635904, 1)))
```




    MinMaxScaler(copy=True, feature_range=(0, 1))




```python
%%time
trainloc = 'data/2019-12-21T215926Z/train.h5'
#X = read_h5_raw(f'data/2019-12-21T215926Z/train.h5', )
scaler, params_vec = build_scaler_from_h5(
                     trainloc,
                     datasets=[x for x in h5_keys(trainloc) 
                               if '_X' in x],
                     label=0
                     )

```

    100%|██████████| 45/45 [00:11<00:00,  4.00it/s]

    CPU times: user 426 ms, sys: 609 ms, total: 1.04 s
    Wall time: 11.3 s


    



```python
#b = np.array(list(zip(*params_vec)))
plt.plot(params_vec)
plt.title(f'min/max for label={0}')
```




    Text(0.5, 1.0, 'min/max for label=0')




![png](2019-12-21_files/2019-12-21_14_1.png)



```python
# mysterious jump we have there huh.
```


```python
%%time
trainloc = 'data/2019-12-21T215926Z/train.h5'
#X = read_h5_raw(f'data/2019-12-21T215926Z/train.h5', )
outvec = [
    build_scaler_from_h5(
                     trainloc,
                     datasets=[x for x in h5_keys(trainloc) 
                               if '_X' in x],
                     feature=i)
    for i in range(8)]
scalers = {i: outvec[i][0] for i in range(8)}
```

    100%|██████████| 45/45 [00:00<00:00, 52.96it/s]
    100%|██████████| 45/45 [00:00<00:00, 60.76it/s]
    100%|██████████| 45/45 [00:00<00:00, 60.51it/s]
    100%|██████████| 45/45 [00:00<00:00, 60.36it/s]
    100%|██████████| 45/45 [00:00<00:00, 61.46it/s]
    100%|██████████| 45/45 [00:00<00:00, 59.52it/s]
    100%|██████████| 45/45 [00:00<00:00, 60.01it/s]
    100%|██████████| 45/45 [00:00<00:00, 59.29it/s]

    CPU times: user 2.7 s, sys: 3.38 s, total: 6.08 s
    Wall time: 6.1 s


    



```python
fig = plt.figure(figsize=(12, 14))
for i in range(8):
    params_vec = outvec[i][1]
    ax = fig.add_subplot(int('42' + str(i+1)))
    ax.plot(params_vec)
    ax.set(title=(f'min/max for feature={i}'))


# fig = plt.figure(figsize=(12,14))
# for i in range(8):
#     ax = fig.add_subplot(int('42' + str(i+1)))
#     ax.hist(X[:,0,i], bins=50)
#     ax.set(title=f'feature {i} hist')
    
```


![png](2019-12-21_files/2019-12-21_17_0.png)



```python
# Ok cool. Glad I also ended up doing this on the pre-scaled data. 
# So next I'll be able to look at the same roughly for the post scaled.
```


```python
trainloc = 'data/2019-12-21T215926Z/train.h5'
first = [x for x in h5_keys(trainloc) 
                               if '_X' in x][0]
feature = 0
X = read_h5_raw(f'data/2019-12-21T215926Z/train.h5', first)
fullsize = X.shape[0]*X.shape[1]
b = np.reshape(X[:, :, feature].ravel(),
                       (fullsize, 1))
c = np.histogram(b)
```


```python
print(c[0], c[1])
print(b.shape)
```

    [ 12672  44928  56694  63339  56898  50170  51687  62888 108628 128000] [662.062012  662.4834109 662.9048098 663.3262087 663.7476076 664.1690065
     664.5904054 665.0118043 665.4332032 665.8546021 666.276001 ]
    (635904, 1)



```python
plt.plot(c[0])
```




    [<matplotlib.lines.Line2D at 0x7f8505faa4e0>]




![png](2019-12-21_files/2019-12-21_21_1.png)



```python
plt.plot(b)
```




    [<matplotlib.lines.Line2D at 0x7f8506052908>]




![png](2019-12-21_files/2019-12-21_22_1.png)



```python
vars(scalers[0])
```




    {'feature_range': (0, 1),
     'copy': True,
     'n_samples_seen_': 28444928,
     'scale_': array([0.00567749]),
     'min_': array([-3.66065535]),
     'data_min_': array([644.765991]),
     'data_max_': array([820.900024]),
     'data_range_': array([176.134033])}




```python
# After scaler..
plt.plot(scalers[0].transform(b))
```




    [<matplotlib.lines.Line2D at 0x7f8505fce9e8>]




![png](2019-12-21_files/2019-12-21_24_1.png)



```python
X = np.array([[-1, 2, 4], [-0.5, 6, 2], [0, 10, 3], [1, 18, 2]])
scaler = MinMaxScaler()
print(scaler.fit(X))

print(scaler.data_min_, scaler.data_max_)

print(scaler.transform(X))
# X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))



```

    MinMaxScaler(copy=True, feature_range=(0, 1))
    [-1.  2.  2.] [ 1. 18.  4.]
    [[0.   0.   1.  ]
     [0.25 0.25 0.  ]
     [0.5  0.5  0.5 ]
     [1.   1.   0.  ]]



```python
(X[:,2] - 2)/2
```




    array([1. , 0. , 0.5, 0. ])



#### Multi scaler instead
So I don't have to cut up/ stitch back together 


```python
# print(first)
X = read_h5_raw(f'data/2019-12-21T215926Z/train.h5', first)
print(X.shape)
print(np.reshape(X[:10, :, :], (10*X.shape[1], 8)).shape)
ten = 10
scaler = MinMaxScaler()
scaler.fit(np.reshape(X[:10, :, :], (10*X.shape[1], 8)))
print(scaler.transform(np.reshape(X[:10, :, :], (10*X.shape[1], 8))))
```

    (9936, 64, 8)
    (640, 8)
    [[0.         0.46141588 0.         ... 0.76985984 0.44254933 0.52833478]
     [0.         0.46141588 0.         ... 0.69486598 0.31096341 0.69697528]
     [0.         0.46141588 0.         ... 0.32758439 0.6828147  0.75729886]
     ...
     [1.         1.         1.         ... 0.48684134 0.42603425 0.38297394]
     [1.         1.         1.         ... 0.77061457 0.93265624 0.57005549]
     [1.         1.         1.         ... 0.70745753 0.80937469 0.30836016]]



```python
# Ok cool... so this reshape seems to be doing what I need..
# It is conserving the dimensionality somehow the way I want it.
Xa = np.vectorize(int)(X[:3,:4,3:5])
print(Xa.shape)
print(Xa)
print(np.reshape(Xa, (12,2)))
```

    (3, 4, 2)
    [[[1 8]
      [1 7]
      [1 1]
      [3 0]]
    
     [[1 7]
      [1 1]
      [3 0]
      [0 0]]
    
     [[1 1]
      [3 0]
      [0 0]
      [4 4]]]
    [[1 8]
     [1 7]
     [1 1]
     [3 0]
     [1 7]
     [1 1]
     [3 0]
     [0 0]
     [1 1]
     [3 0]
     [0 0]
     [4 4]]



```python
# Next:
# Oh wow that's perfect ! ... So I can use this to first create a 
# Standard scaler, across the whole dataset i have. perfect.

def build_many_scalers_from_h5(loc, datasets):
    # one label at a time here..
    scaler = MinMaxScaler()
    params_vec = []
    for name in tqdm(datasets):
        X = read_h5_raw(loc, name)
        fullsize = X.shape[0]*X.shape[1]
        scaler.partial_fit(
            np.reshape(X,
                       (fullsize, X.shape[2])))
        params_vec.append(
            [scaler.data_min_, scaler.data_max_])

    return scaler, params_vec

def apply_scalers(source_loc, datasets):
    # Apply using a suffix, '_scaled'
    pass
    # for each dataset, read. then write '_scaled'
```


```python
# trainloc = 'data/2019-12-21T215926Z/train.h5'
output = build_many_scalers_from_h5(
                        trainloc, 
                        datasets=[x for x in h5_keys(trainloc) 
                               if '_X' in x]
)

```

    100%|██████████| 45/45 [00:03<00:00, 11.31it/s]



```python
# Ok for the below, looks like this other scaling 
# is working the same way cool..
themin, themax = 0, 1

fig = plt.figure(figsize=(12, 14))
#for i in range(8):


for col in range(8):
    #params_vec = outvec[i][1]
    ax = fig.add_subplot(int('42' + str(col+1)))
    #ax.plot(params_vec)
    ax.set(title=(f'min/max for feature={col}'))

    ax.plot(
        [output[1][i][themin][col] for i in range(len(output[1]))]
    
    )
    ax.plot(
    [output[1][i][themax][col] for i in range(len(output[1]))]
    )
```


![png](2019-12-21_files/2019-12-21_32_0.png)



```python

```
