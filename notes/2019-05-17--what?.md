### What
- Summarizing what happened [yesterday](https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-05-15-aviation-rnn.ipynb)
- [First train](#initial-attempt-was-missing-a-lot-of-data)
- [Take two](#take-two)
- [Take two, output](#2-model-run-output)
- [Model-3, on Sagemaker notebook](#model-3-model-2-config-also-run-on-a-sagemaker-notebook)
- [Model-3, output](#this-model-3-is-taking-a-bit-longer)


#### Initial attempt was missing a lot of data
* (call this `model-1`)
```python
# First model train, 
%time out, model = runner()
# => 

Start make_data 2019-05-16 13:41:08
Start building training set 2019-05-16 13:41:08
Start building testing set 2019-05-16 13:41:09
Start bake_model 2019-05-16 13:41:10
Train... 2019-05-16 13:41:11
Train on 91874 samples, validate on 91876 samples
Epoch 1/2
 - 874s - loss: 0.1881 - acc: 0.1129 - val_loss: 1.0480e-07 - val_acc: 0.2500
Epoch 2/2
 - 893s - loss: 0.1271 - acc: 0.0466 - val_loss: 1.0480e-07 - val_acc: 0.0000e+00
CPU times: user 1h 18min 13s, sys: 32min 39s, total: 1h 50min 53s
Wall time: 29min 30s
```
* I looked at the output classes, and crap.. but good thing it only took `29 min`
* The reason why was i have a bug in `get_windows()` , I forgot to cycle through all of the choices.
```python
# - ok darn looks like the input data looks corrupt since theres just one class represented
print('Counter y test original', Counter(outdata['y_test_original']))
print('Counter predict classes', Counter(predict_classes))
print(len(Counter(outdata['y_test_original'])))
print (outdata['y_test'].shape)
Counter y test original Counter({'C': 91876})
Counter predict classes Counter({2: 91876})
```
```python
# There are not enough sequences. Only ~91876 , 
# - but should be  around  447652 - 256 . 
#
# ipdb> pp choices
# ([1], [0, 1], ['CA', 'DA', 'SS'])
# 
# ipdb> pp crew, seat, experiment
# (1, 0, 'CA')
#
# - so only the first choice was being used to build sequences.
```

#### Take two
* Okay, After fixing that bug and adding an assertion to prevent that in the future
* So now with the particular `make_data` call below, specifying `percent_of_data=1`, 
the metadata shows `A, B, C and D` are all represented.
```python
In [325]:
# ...after some more troubleshooting...
with ipdb.launch_ipdb_on_exception():
    outdata = make_data(df, crews={'training': [1],
                        'test': [2]},
              sequence_window=256, percent_of_data=1,
             feature_cols={'r': simple_scaler})
Start building training set 2019-05-16 17:06:35 EST
Start building testing set 2019-05-16 17:06:40 EST
In [326]:
outdata['metadata']
Out[326]:
{'output': {'shapes': {'x_train': (446110, 256, 1),
   'y_train': (446110, 4),
   'x_test': (551326, 256, 1),
   'y_test': (551326, 4),
   'y_train_original': (446110,),
   'y_test_original': (551326,),
   'traindf': (447652, 7),
   'testdf': (552868, 7)},
  "Counter(outdata['y_train_original'])": {'A': 234352,
   'C': 180851,
   'D': 23218,
   'B': 7689},
  "Counter(outdata['y_test_original'])": {'C': 183718,
   'A': 325198,
   'D': 27039,
   'B': 15371}},
 'input': {'kwargs': {'percent_of_data': 1,
   'sequence_window': 256,
   'feature_cols': ['r']}},
 'data_ts': '2019-05-16 17:06:47 EST'}
 ```

##### #2 model run output,
```python
# Second model train, 
%time out, model = runner()
# =>

Start make_data 2019-05-16 17:23:49 EST
Start building training set 2019-05-16 17:23:49 EST
Start building testing set 2019-05-16 17:23:54 EST
Start bake_model 2019-05-16 17:24:00 EST
Train... 2019-05-16 17:24:01 EST
Train on 446110 samples, validate on 551326 samples
Epoch 1/2
 - 4870s - loss: 0.4643 - acc: 0.7668 - val_loss: 0.4926 - val_acc: 0.7949
Epoch 2/2
 - 5335s - loss: 0.4538 - acc: 0.7738 - val_loss: 0.5151 - val_acc: 0.7229
CPU times: user 7h 10min 45s, sys: 2h 56min 47s, total: 10h 7min 33s
Wall time: 2h 50min 30s
```
* Wow that's a long time. My mac *memory* was showing pegged at `~8GB ` compressed memory for the whole time. 
_So if it's compressed at least that is supposed be helpful, but when I look at Real Memory (RMEM) and private (RPRVT) 
they are 17MB and 172KB , so maybe that memory is a bottle neck?_
* So the `~551k (551326)` many `256-long` sequences in `x_test` took _(below)_ `12min` to predict.
```python
%time allpreds = model.predict(out['x_test'])
# => 

CPU times: user 37min 35s, sys: 12min 40s, total: 50min 15s
Wall time: 12min 57s
```
* Hmm.. and only the two dominant classes are represented in the output.
```python
allpreds_predict_classes = np.argmax(allpreds, axis=1)
print('Counter predict classes', Counter(allpreds_predict_classes))
print('original', Counter(out['y_test_original']))
# => 
Counter predict classes Counter({2: 275687, 0: 275639})
original Counter({'A': 325198, 'C': 183718, 'D': 27039, 'B': 15371})
```
* they make up `92%` of the sequences here, 
```python
original = dict(Counter(out['y_test_original']))
print((original['A'] + original['C'])/sum(original.values()))
# =>
0.9230763649818801
````

#### model-3: model-2 config also run on a sagemaker notebook 
* This job is [still running](https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-05-16-sagemaker-book.ipynb) , but saved the `80%` done result, just `3.2 more hours` to go heh. :smiley: _( as of 2019-05-17 16:20 UTC)_
* The notebook instance is a `ml.p2.xlarge` , which means it has these [specs](https://aws.amazon.com/sagemaker/pricing/instance-types/) :
```
Instance type	vCPU 	GPU 	Mem (GiB) 	GPU Mem (GiB) 	Network Performance
ml.p2.xlarge  	4   	1xK80 	61 	12 	High
```
* My laptop is a `2012` 
```
  Model Name:	MacBook Pro
  Model Identifier:	MacBookPro10,1
  Processor Name:	Intel Core i7
  Processor Speed:	2.6 GHz
  Number of Processors:	1
  Total Number of Cores:	4
  L2 Cache (per Core):	256 KB
  L3 Cache:	6 MB
  Memory:	8 GB
  
# graphics:

  Chipset Model:	NVIDIA GeForce GT 650M
  Type:	GPU
  Bus:	PCIe
  PCIe Lane Width:	x8
  VRAM (Total):	1 GB
  Automatic Graphics Switching:	Supported
  gMux Version:	3.2.19 [3.2.8]
  Metal:	Supported, feature set macOS GPUFamily1 v4
```
* So I don't get it, if this instance has way more memory and presumably a better GPU, what's going on here?

##### this model-3 is taking a bit longer..
```python
# Third  model train, 
%time out, model = runner()
# =>
Start make_data 2019-05-17 01:13:03  UTC
Start building training set 2019-05-17 01:13:03 UTC
Start building testing set 2019-05-17 01:13:07 UTC
metadata {'output': {'shapes': {'x_train': (446110, 256, 1), 'y_train': (446110, 4), 'x_test': (551326, 256, 1), 'y_test': (551326, 4), 'y_train_original': (446110,), 'y_test_original': (551326,), 'traindf': (447652, 7), 'testdf': (552868, 7)}, "Counter(outdata['y_train_original'])": {'A': 234352, 'C': 180851, 'D': 23218, 'B': 7689}, "Counter(outdata['y_test_original'])": {'C': 183718, 'A': 325198, 'D': 27039, 'B': 15371}}, 'input': {'kwargs': {'crews': {'training': [1], 'test': [2]}, 'percent_of_data': 1, 'sequence_window': 256, 'feature_cols': ['r']}}, 'data_ts': '2019-05-17 01:13:12 UTC'}
Start bake_model 2019-05-17 01:13:12 UTC

Train on 446110 samples, validate on 551326 samples
Epoch 1/10
 - 6575s - loss: 0.4677 - acc: 0.7747 - val_loss: 0.5016 - val_acc: 0.7502
Epoch 2/10
 - 6553s - loss: 0.4582 - acc: 0.7668 - val_loss: 0.4974 - val_acc: 0.7946
Epoch 3/10
 - 6531s - loss: 0.4564 - acc: 0.7688 - val_loss: 0.4944 - val_acc: 0.7949
Epoch 4/10
 - 6498s - loss: 0.4492 - acc: 0.7788 - val_loss: 0.4997 - val_acc: 0.7677
Epoch 5/10
 - 6492s - loss: 0.4048 - acc: 0.8029 - val_loss: 0.4977 - val_acc: 0.7669
Epoch 6/10
 - 6483s - loss: 0.4045 - acc: 0.7974 - val_loss: 0.4971 - val_acc: 0.7667
Epoch 7/10
 - 6486s - loss: 0.3763 - acc: 0.8342 - val_loss: 0.4924 - val_acc: 0.7945
Epoch 8/10
 - 6513s - loss: 0.3293 - acc: 0.8400 - val_loss: 1.8344 - val_acc: 0.6459
Epoch 9/10
... # this is still running 
```
* So the `ml.p2.xlarge` is taking `6513s ~ 1.8 hours/epoch ` and my laptop was taking `4870s-5335s , 1.4-1.5 hours /epoch`
