#### Summary
- Tried a different initializer today . Hmm Not super help ful though.
- Looks like the default initializer, ( which is aka Xavier Uniform), among what is there ,  https://www.tensorflow.org/api_docs/python/tf/keras/initializers  , per what I am seeing below, is indeed still ranom. 
- But here, I tried the Xavier Normal instead for fun..

- So for sure it seems its worth considering this as another hyper parameter
, https://adventuresinmachinelearning.com/weight-initialization-tutorial-tensorflow/


```python

from importlib import reload
import os
import pandas as pd
from io import StringIO
import itertools
import ipdb
import datetime
from collections import Counter

import h5py
import json
import tensorflow as tf
from tensorflow import keras

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import joblib
print(tf.__version__)

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Embedding
from keras.layers import LSTM

from keras.callbacks import EarlyStopping

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
import mytf.s3utils as msu
import mytf.utils as mu
import mytf.validation as mv
import mytf.plot as mp
```

    1.14.0


    Using TensorFlow backend.



```python
tf.enable_eager_execution()

```


```python
def bake_model():
    lstm_params = [{
        'units': 64,
        'dropout': 0.6,
        'recurrent_dropout': 0.6,
        'batch_input_shape': (None, 64, 8),
        },

    ]

    optimizer_params = {
        'learning_rate': 0.001,  
        'beta1': 0.9, 
        'beta2': 0.999, 
        'epsilon': 1e-08
    }

    model = tf.keras.Sequential([
        tf.keras.layers.LSTM(**lstm_params[0]),
        # 4 because 'A', 'B', 'C', 'D'.
        tf.keras.layers.Dense(4)])
    return model
```


```python
def weights(obj, ):
    return [x for x in vars(obj).keys() if   'weight' in x]
# LSTM weights...
# ['_trainable_weights', '_non_trainable_weights', '_initial_weights']

#print(model._layers[1]._initial_weights) # <- None
# print([weights(x) for x in
#    model._layers[1]._layers])
# [['_trainable_weights', '_non_trainable_weights', '_initial_weights'], [], []]

#[x.shape for x in model._layers[1]._layers[0]._trainable_weights]
# [TensorShape([Dimension(8), Dimension(256)]),
# TensorShape([Dimension(64), Dimension(256)]),
# TensorShape([Dimension(256)])]

# model._layers[1]._layers[0]._initial_weights  # <--None, hmm..

for i in range(3):
    model = bake_model()
    print([x[0, :5]  for x in model._layers[1]._layers[0]._trainable_weights[:2]])
    print(model._layers[1]._layers[0]._trainable_weights[2])
```

    [<tf.Tensor: id=1324, shape=(5,), dtype=float32, numpy=
    array([-0.03004508,  0.14491767,  0.04427765,  0.08059219,  0.03595215],
          dtype=float32)>, <tf.Tensor: id=1329, shape=(5,), dtype=float32, numpy=
    array([ 0.11298072,  0.0676304 ,  0.02081808,  0.02089654, -0.01962803],
          dtype=float32)>]
    <tf.Variable 'lstm_1/bias:0' shape=(256,) dtype=float32, numpy=
    array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0.], dtype=float32)>
    [<tf.Tensor: id=1964, shape=(5,), dtype=float32, numpy=
    array([0.11387375, 0.01473439, 0.06241684, 0.02221012, 0.04531623],
          dtype=float32)>, <tf.Tensor: id=1969, shape=(5,), dtype=float32, numpy=
    array([-0.04654741, -0.03044676, -0.00881757,  0.05438439,  0.01528069],
          dtype=float32)>]
    <tf.Variable 'lstm_2/bias:0' shape=(256,) dtype=float32, numpy=
    array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0.], dtype=float32)>
    [<tf.Tensor: id=2604, shape=(5,), dtype=float32, numpy=
    array([ 0.11252353, -0.07627354,  0.12438098, -0.09523362,  0.14129129],
          dtype=float32)>, <tf.Tensor: id=2609, shape=(5,), dtype=float32, numpy=
    array([ 0.00222719,  0.06134533, -0.0226919 , -0.0752966 ,  0.11333336],
          dtype=float32)>]
    <tf.Variable 'lstm_3/bias:0' shape=(256,) dtype=float32, numpy=
    array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0.], dtype=float32)>



```python
# Hmm so part of that looks random each time ^^ 
# but that third element doesnt look random.
```


```python
vars(model._layers[1]._layers[2])
```




    {'_non_append_mutation': False,
     '_external_modification': False,
     '_self_trainable': True,
     '_self_extra_variables': [],
     '_storage': [InputSpec(shape=[None, 64], ndim=2),
      InputSpec(shape=[None, 64], ndim=2)],
     '_last_wrapped_list_snapshot': [InputSpec(shape=[None, 64], ndim=2),
      InputSpec(shape=[None, 64], ndim=2)],
     '_self_setattr_tracking': True,
     '_self_unconditional_checkpoint_dependencies': [],
     '_self_unconditional_dependency_names': {},
     '_self_unconditional_deferred_dependencies': {},
     '_self_update_uid': -1,
     '_self_name_based_restores': set()}




```python
model._layers[1].kernel_initializer
```




    <tensorflow.python.ops.init_ops.GlorotUniform at 0x7f75a09b73c8>




```python
# Ok. so I see this ^ GlorotUniform at 
# https://www.tensorflow.org/api_docs/python/tf/keras/initializers
#  ( which is aka Xavier Uniform)
# So for sure it seems its worth considering this as another hyper parameter
# So since I've been using this default ^, I want to try the 
# Xavier Normal option noted here, https://adventuresinmachinelearning.com/weight-initialization-tutorial-tensorflow/
# 

```


```python


```


```python
# Working dir... for new model
save_dir = 'history'
ts = mu.quickts(); print('starting,', ts)

workdir = f'{save_dir}/{ts}'
os.mkdir(workdir)
print(f'Made new workdir, {workdir}')
```

    starting, 2020-01-12T225750Z
    Made new workdir, history/2020-01-12T225750Z



```python
def bake_model():
    lstm_params = [{
        'units': 64,
        'dropout': 0.6,
        'recurrent_dropout': 0.6,
        'batch_input_shape': (None, 64, 8),
        'kernel_initializer': tf.initializers.glorot_normal() # GlorotNormal()
                               #tf.initializers.he_normal()
        },

    ]

    optimizer_params = {
        'learning_rate': 0.001,  
        'beta1': 0.9, 
        'beta2': 0.999, 
        'epsilon': 1e-08
    }

    model = tf.keras.Sequential([
        tf.keras.layers.LSTM(**lstm_params[0]),
        # 4 because 'A', 'B', 'C', 'D'.
        tf.keras.layers.Dense(4)])
    return model
```


```python
model = bake_model()
```


```python
BATCH_SIZE = 32
EPOCHS = 1
# Use datasets from 
# 2019-12-25.ipynb
# 
# train ... new datasets, ...
datadir = 'history/2019-12-22T174803Z'
# train_loc = f'{datadir}/train_balanced.h5'
test_loc = f'{datadir}/test_balanced.h5'
train_shuff_loc = f'{datadir}/train_scaled_balanced_shuffled.h5'
print(mu.h5_keys(train_shuff_loc))
print(mu.h5_keys(test_loc))

X, Ylabels = mu.read_h5_two(
                source_location=train_shuff_loc, 
                Xdataset=f'X',
                Ydataset=f'Ylabels')
size = X.shape[0]

# save base unfitted model.
mu.save_model(model=model, 
              loc=f'{workdir}/00000__unfitted_model.h5')

```

    ['X', 'Ylabels']
    ['X_0', 'X_1', 'X_2', 'X_3', 'Ylabels_0', 'Ylabels_1', 'Ylabels_2', 'Ylabels_3']



```python
%%time

model = mu.load_model(f'{workdir}/00000__unfitted_model.h5')

class_weights = {0: 1., 1: 1., 2: 1., 3: 1.}
dataset_batches = mu.build_dataset_weighty_v3(
        {'x_train': X,
         'ylabels_train': Ylabels.astype('int64')},
        list(range(size)), 
        class_weights,
        batch_size=BATCH_SIZE)
    
#with ipdb.launch_ipdb_on_exception():
mu.do_train( 
        model,
        dataset_batches,
        k=size,
        epochs=EPOCHS,
        optimizer_params=optimizer_params,
        saveloc=workdir)
```

    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.



    CPU times: user 6min 53s, sys: 986 ms, total: 6min 54s
    Wall time: 6min 55s


    



```python
print(list(np.arange(0, 1100, 50)))
```

    [0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050]



```python
epoch = 0
for batch in tqdm(list(np.arange(0, 1100, 50))):
    step = batch
    prefix = (f'{workdir}/epoch_{str(epoch).zfill(3)}'
                           f'_batch_{str(batch).zfill(5)}')

    modelname = f'{prefix}_model.h5'
    print(modelname, os.path.exists(modelname))
```

    
    100%|██████████| 22/22 [00:00<00:00, 14902.24it/s]

    history/2020-01-12T225750Z/epoch_000_batch_00000_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_00050_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_00100_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_00150_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_00200_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_00250_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_00300_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_00350_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_00400_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_00450_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_00500_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_00550_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_00600_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_00650_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_00700_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_00750_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_00800_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_00850_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_00900_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_00950_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_01000_model.h5 True
    history/2020-01-12T225750Z/epoch_000_batch_01050_model.h5 True


    



```python
print('starting validation', mu.quickts())
batch_losses_vec = []

epoch = 0
for batch in tqdm(list(np.arange(0, 1100, 50))):
    step = batch
    prefix = (f'{workdir}/epoch_{str(epoch).zfill(3)}'
                           f'_batch_{str(batch).zfill(5)}')

    modelname = f'{prefix}_model.h5'
    print(modelname, os.path.exists(modelname))

    steploss = mv.perf_wrapper(modelname,
                               dataloc=test_loc,
                               eager=True,
                              batch_size=32)
    batch_losses_vec.append([float(x) for x in steploss])
    mv.json_save({'batch_losses_vec': batch_losses_vec,
                  'step': int(step)
              }, 
              f'{prefix}_validation_losses.json')
    
print('done validation', mu.quickts())
#####
lossesarr = np.array(batch_losses_vec)
meanlossesarr = np.mean(lossesarr, axis=1)

batch_losses_vec[:5]
#batch_losses_vec = []
#for step in np.arange(0, 1068, 10):
# [2.8359528, 0.45356295, 1.7049086, 4.099845]

plt.plot([x[0] for x in batch_losses_vec], color='blue', label='0')
plt.plot([x[1] for x in batch_losses_vec], color='green', label='1')
plt.plot([x[2] for x in batch_losses_vec], color='red', label='2')
plt.plot([x[3] for x in batch_losses_vec], color='orange', label='3')
plt.plot(meanlossesarr, color='black', label='mean')
plt.title(f'validation losses  (model {ts})')
plt.legend()     
        
```

    
      0%|          | 0/22 [00:00<?, ?it/s][A

    starting validation 2020-01-12T233154Z
    history/2020-01-12T225750Z/epoch_000_batch_00000_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
      5%|▍         | 1/22 [05:42<1:59:52, 342.52s/it][A

    history/2020-01-12T225750Z/epoch_000_batch_00050_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
      9%|▉         | 2/22 [11:22<1:53:55, 341.79s/it][A

    history/2020-01-12T225750Z/epoch_000_batch_00100_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
     14%|█▎        | 3/22 [17:03<1:48:06, 341.40s/it][A

    history/2020-01-12T225750Z/epoch_000_batch_00150_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
     18%|█▊        | 4/22 [22:47<1:42:43, 342.41s/it][A

    history/2020-01-12T225750Z/epoch_000_batch_00200_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
     23%|██▎       | 5/22 [28:32<1:37:12, 343.06s/it][A

    history/2020-01-12T225750Z/epoch_000_batch_00250_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
     27%|██▋       | 6/22 [34:33<1:32:56, 348.54s/it][A

    history/2020-01-12T225750Z/epoch_000_batch_00300_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
     32%|███▏      | 7/22 [40:20<1:27:01, 348.09s/it][A

    history/2020-01-12T225750Z/epoch_000_batch_00350_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
     36%|███▋      | 8/22 [46:01<1:20:42, 345.87s/it][A

    history/2020-01-12T225750Z/epoch_000_batch_00400_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
     41%|████      | 9/22 [51:46<1:14:51, 345.52s/it][A

    history/2020-01-12T225750Z/epoch_000_batch_00450_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
     45%|████▌     | 10/22 [57:32<1:09:08, 345.69s/it][A

    history/2020-01-12T225750Z/epoch_000_batch_00500_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
     50%|█████     | 11/22 [1:03:13<1:03:08, 344.43s/it][A

    history/2020-01-12T225750Z/epoch_000_batch_00550_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
     55%|█████▍    | 12/22 [1:08:56<57:18, 343.86s/it]  [A

    history/2020-01-12T225750Z/epoch_000_batch_00600_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
     59%|█████▉    | 13/22 [1:14:40<51:34, 343.88s/it][A

    history/2020-01-12T225750Z/epoch_000_batch_00650_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
     64%|██████▎   | 14/22 [1:20:25<45:53, 344.19s/it][A

    history/2020-01-12T225750Z/epoch_000_batch_00700_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
     68%|██████▊   | 15/22 [1:26:10<40:12, 344.59s/it][A

    history/2020-01-12T225750Z/epoch_000_batch_00750_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
     73%|███████▎  | 16/22 [1:31:51<34:21, 343.58s/it][A

    history/2020-01-12T225750Z/epoch_000_batch_00800_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
     77%|███████▋  | 17/22 [1:37:32<28:34, 342.81s/it][A

    history/2020-01-12T225750Z/epoch_000_batch_00850_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
     82%|████████▏ | 18/22 [1:43:14<22:50, 342.55s/it][A

    history/2020-01-12T225750Z/epoch_000_batch_00900_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
     86%|████████▋ | 19/22 [1:48:54<17:05, 341.72s/it][A

    history/2020-01-12T225750Z/epoch_000_batch_00950_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
     91%|█████████ | 20/22 [1:54:35<11:23, 341.53s/it][A

    history/2020-01-12T225750Z/epoch_000_batch_01000_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
     95%|█████████▌| 21/22 [2:00:17<05:41, 341.72s/it][A

    history/2020-01-12T225750Z/epoch_000_batch_01050_model.h5 True
    WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.


    
    100%|██████████| 22/22 [2:06:00<00:00, 343.64s/it][A

    done validation 2020-01-13T013754Z


    





    <matplotlib.legend.Legend at 0x7f75709c09e8>




![png](2020-01-12_files/2020-01-12_16_48.png)

