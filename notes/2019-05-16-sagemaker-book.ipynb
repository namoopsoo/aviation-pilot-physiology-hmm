{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import os\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_s3_resource():\n",
    "    s3 = boto3.resource('s3',\n",
    "            # aws_access_key_id=os.getenv('S3_BLOG_UPLOAD_ACCESS_KEY'),\n",
    "            # aws_secret_access_key=os.getenv('S3_BLOG_UPLOAD_SECRET'),\n",
    "            region_name='us-east-1')\n",
    "    return s3\n",
    "\n",
    "\n",
    "def write_s3_file(bucket_name, s3_filename, content):\n",
    "    assert isinstance(content, basestring)\n",
    "    s3conn = make_s3_resource()\n",
    "    try:\n",
    "        s3conn.Object(bucket_name, s3_filename).put(\n",
    "                Body=content)\n",
    "        return True\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        return False\n",
    "\n",
    "\n",
    "def read_s3_file(bucket_name, s3_filename):\n",
    "    s3conn = make_s3_resource()\n",
    "    # try:\n",
    "    return s3conn.Object(bucket_name, s3_filename).get()[\"Body\"].read()\n",
    "    # except botocore.exceptions.ClientError as e:\n",
    "\n",
    "\n",
    "def s3_csv_to_df(bucket_name, s3_filename):\n",
    "    blah = read_s3_file(bucket_name, s3_filename)\n",
    "    foo = StringIO(blah.decode(\"utf-8\"))\n",
    "    return pd.read_csv(foo)\n",
    "\n",
    "def big_s3_csv_to_df(bucket_name, s3_filename_prefix, suffixes):\n",
    "    filenames = [s3_filename_prefix + suff\n",
    "            for suff in suffixes]\n",
    "    # return filenames\n",
    "    parts = [read_s3_file(bucket_name, s3_filename) \n",
    "            for s3_filename in filenames ]\n",
    "    blah = functools.reduce(lambda x, y: x+y, parts)\n",
    "    foo = StringIO(blah.decode(\"utf-8\"))\n",
    "    return pd.read_csv(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.4 s, sys: 9.24 s, total: 37.7 s\n",
      "Wall time: 54.4 s\n"
     ]
    }
   ],
   "source": [
    "blah = read_s3_file('my-sagemaker-blah',\n",
    "         'aviation/small.csv')\n",
    "# df = s3_csv_to_df('my-sagemaker-blah', 'aviation/small.csv')\n",
    "\n",
    "\n",
    "suffixes = [\n",
    "    '.part1' + letter\n",
    "     for letter in ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n",
    "]\n",
    "\n",
    "%time df = big_s3_csv_to_df('my-sagemaker-blah', 'aviation/sorted_train.csv', suffixes)\n",
    "# 'sorted_train.csv.part1a'\n",
    "# foo = StringIO(blah.decode(\"utf-8\"))\n",
    "# print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4867421, 28)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start make_data 2019-05-17 01:13:03 EST\n",
      "Start building training set 2019-05-17 01:13:03 EST\n",
      "Start building testing set 2019-05-17 01:13:07 EST\n",
      "metadata {'output': {'shapes': {'x_train': (446110, 256, 1), 'y_train': (446110, 4), 'x_test': (551326, 256, 1), 'y_test': (551326, 4), 'y_train_original': (446110,), 'y_test_original': (551326,), 'traindf': (447652, 7), 'testdf': (552868, 7)}, \"Counter(outdata['y_train_original'])\": {'A': 234352, 'C': 180851, 'D': 23218, 'B': 7689}, \"Counter(outdata['y_test_original'])\": {'C': 183718, 'A': 325198, 'D': 27039, 'B': 15371}}, 'input': {'kwargs': {'crews': {'training': [1], 'test': [2]}, 'percent_of_data': 1, 'sequence_window': 256, 'feature_cols': ['r']}}, 'data_ts': '2019-05-17 01:13:12 EST'}\n",
      "Start bake_model 2019-05-17 01:13:12 EST\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train... 2019-05-17 01:13:13 EST\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 446110 samples, validate on 551326 samples\n",
      "Epoch 1/10\n",
      " - 6575s - loss: 0.4677 - acc: 0.7747 - val_loss: 0.5016 - val_acc: 0.7502\n",
      "Epoch 2/10\n",
      " - 6553s - loss: 0.4582 - acc: 0.7668 - val_loss: 0.4974 - val_acc: 0.7946\n",
      "Epoch 3/10\n",
      " - 6531s - loss: 0.4564 - acc: 0.7688 - val_loss: 0.4944 - val_acc: 0.7949\n",
      "Epoch 4/10\n",
      " - 6498s - loss: 0.4492 - acc: 0.7788 - val_loss: 0.4997 - val_acc: 0.7677\n",
      "Epoch 5/10\n",
      " - 6492s - loss: 0.4048 - acc: 0.8029 - val_loss: 0.4977 - val_acc: 0.7669\n",
      "Epoch 6/10\n",
      " - 6483s - loss: 0.4045 - acc: 0.7974 - val_loss: 0.4971 - val_acc: 0.7667\n",
      "Epoch 7/10\n",
      " - 6486s - loss: 0.3763 - acc: 0.8342 - val_loss: 0.4924 - val_acc: 0.7945\n",
      "Epoch 8/10\n",
      " - 6513s - loss: 0.3293 - acc: 0.8400 - val_loss: 1.8344 - val_acc: 0.6459\n",
      "Epoch 9/10\n"
     ]
    }
   ],
   "source": [
    "# Third  model train, \n",
    "%time out, model = runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start bake_model 2019-05-17 01:13:12 UTC \n",
    "# Train on 446110 samples, validate on 551326 samples\n",
    "epochs = [6575, 6553, 6531, 6498, 6492, 6483, 6486]\n",
    "# \n",
    "# In [28]: start + datetime.timedelta(seconds=sum(epochs))                                                                                    \n",
    "# Out[28]: datetime.datetime(2019, 5, 17, 13, 53, 30, tzinfo=<UTC>)\n",
    "# ==> so Epoch 8 started around ^^ . \n",
    "# So 8 should finish around \n",
    "# In [33]: start + datetime.timedelta(seconds=sum(epochs)) + datetime.timedelta(seconds=round(sum(epochs)/7))                                 \n",
    "# Out[33]: datetime.datetime(2019, 5, 17, 15, 42, 7, tzinfo=<UTC>)\n",
    "# \n",
    "# and that indeed does happen.\n",
    "\n",
    "'''\n",
    "Epoch 1/10\n",
    " - 6575s - loss: 0.4677 - acc: 0.7747 - val_loss: 0.5016 - val_acc: 0.7502             \n",
    "Epoch 2/10\n",
    " - 6553s - loss: 0.4582 - acc: 0.7668 - val_loss: 0.4974 - val_acc: 0.7946\n",
    "Epoch 3/10\n",
    " - 6531s - loss: 0.4564 - acc: 0.7688 - val_loss: 0.4944 - val_acc: 0.7949\n",
    "Epoch 4/10\n",
    " - 6498s - loss: 0.4492 - acc: 0.7788 - val_loss: 0.4997 - val_acc: 0.7677\n",
    "Epoch 5/10\n",
    " - 6492s - loss: 0.4048 - acc: 0.8029 - val_loss: 0.4977 - val_acc: 0.7669\n",
    "Epoch 6/10\n",
    " - 6483s - loss: 0.4045 - acc: 0.7974 - val_loss: 0.4971 - val_acc: 0.7667\n",
    "Epoch 7/10\n",
    " - 6486s - loss: 0.3763 - acc: 0.8342 - val_loss: 0.4924 - val_acc: 0.7945\n",
    "Epoch 8/10'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay so since there are 9 crews in this data , and indeed I assume the test data \n",
    "#     crews are the same people. Anyway, not too important for now.\n",
    "#     But I can split the data into 6 for train and 3 for testing.\n",
    "#\n",
    "# - So as a preliminary simple model, I want to just use the `r` , the respiration data.\n",
    "# - And I suppose it doesn't really matter all too much if both crew member data\n",
    "#      are intermingled, but I think I will split that away for now.\n",
    "\n",
    "encode_class = np.vectorize(lambda x: {'A': 0,\n",
    "                                      'B': 1,\n",
    "                                      'C': 2,\n",
    "                                      'D': 3}.get(x))\n",
    "\n",
    "simple_scaler = lambda x, a: x*a \n",
    "\n",
    "def timestamp():\n",
    "    return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S EST')\n",
    "\n",
    "def do_standard_scaling(df, cols, scalar_dict=None):\n",
    "    if scalar_dict is None:    \n",
    "        scalar_dict = {col: StandardScaler().fit(df[[col]]) for col in cols}\n",
    "        \n",
    "    for col in cols:\n",
    "        df[col + '_scaled'] = np.resize(\n",
    "            scalar_dict[col].transform(df[[col]]),\n",
    "            (df.shape[0],))\n",
    "    \n",
    "    return scalar_dict, df\n",
    "\n",
    "\n",
    "def make_data(df, crews={'training': [1],\n",
    "                        'test': [2]},\n",
    "              sequence_window=256, percent_of_data=100,\n",
    "             feature_cols={'r': 'standard_scaler'}):\n",
    "\n",
    "    # current sorted as ['crew', 'experiment', 'time']\n",
    "    [0, 1] # each seat\n",
    "    ['CA', 'DA', 'SS'] # experiment\n",
    "    \n",
    "    sort_cols = ['crew', 'seat', 'experiment', 'time']\n",
    "    target_col = 'event'\n",
    "    \n",
    "    what_cols = sort_cols + list(feature_cols) + [target_col]\n",
    "\n",
    "    # Training\n",
    "    traindf = df[df.crew.isin(crews['training'])][what_cols].sort_values(\n",
    "        by=sort_cols).copy()\n",
    "    \n",
    "    scalar_dict, _ = do_standard_scaling(traindf, ['r'])\n",
    "    \n",
    "    print('Start building training set', timestamp())\n",
    "    x_train, y_train = get_windows(traindf, ['r_scaled', 'event'],\n",
    "                                   sequence_window,\n",
    "                                  percent_of_data=percent_of_data)\n",
    "    \n",
    "    # Testing\n",
    "    testdf = df[df.crew.isin(crews['test'])][what_cols].sort_values(\n",
    "        by=sort_cols).copy()\n",
    "\n",
    "    _, _ = do_standard_scaling(testdf, ['r'], scalar_dict)\n",
    "    \n",
    "    \n",
    "    print('Start building testing set', timestamp())\n",
    "    x_test, y_test = get_windows(testdf, ['r_scaled', 'event'],\n",
    "                                 sequence_window,\n",
    "                                 percent_of_data=percent_of_data)\n",
    "\n",
    "\n",
    "    outdata = {\n",
    "        \"x_train\": x_train,\n",
    "        \"y_train\": reshape_y(encode_class(y_train), 4), # y_train,\n",
    "        \"x_test\": x_test,\n",
    "        \"y_test\": reshape_y(encode_class(y_test), 4), # y_test\n",
    "        \"y_train_original\": y_train,\n",
    "        \"y_test_original\": y_test,\n",
    "        \"traindf\": traindf,\n",
    "        \"testdf\": testdf,}\n",
    "    metadata = {\n",
    "        \"metadata\": {\n",
    "            \"output\": {\n",
    "                \"shapes\": {k: outdata[k].shape for k in list(outdata)},\n",
    "                \"Counter(outdata['y_train_original'])\":\n",
    "                dict(Counter(y_train)),\n",
    "                \"Counter(outdata['y_test_original'])\":\n",
    "                dict(Counter(y_test)),},\n",
    "            \"input\": {\"kwargs\": {\n",
    "                \"crews\": crews,\n",
    "                \"percent_of_data\": percent_of_data,\n",
    "                \"sequence_window\": sequence_window,\n",
    "                \"feature_cols\": list(feature_cols)}},\n",
    "            \"data_ts\": timestamp()\n",
    "        }}\n",
    "            \n",
    "    return {**outdata, **metadata}\n",
    "    \n",
    "    \n",
    "def runner():\n",
    "    print('Start make_data', timestamp())\n",
    "    outdata = make_data(df, crews={'training': [1],\n",
    "                        'test': [2]},\n",
    "              sequence_window=256, percent_of_data=1,\n",
    "             feature_cols={'r': simple_scaler})\n",
    "    \n",
    "    validate_data(outdata)\n",
    "    print('metadata', outdata['metadata'])\n",
    "    print('Start bake_model', timestamp())\n",
    "    model = bake_model(**outdata, epochs=10)\n",
    "    return outdata, model\n",
    "\n",
    "def validate_data(data):\n",
    "    assert len(Counter(data['y_train_original'])) > 1\n",
    "    assert len(Counter(data['y_test_original'])) > 1\n",
    "  \n",
    "    \n",
    "def get_windows(df, cols, window_size, percent_of_data=100):\n",
    "    \n",
    "    whats_proportion_index = lambda x, y: round(x*y)\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    choices = (df.crew.unique().tolist(), [0, 1], ['CA', 'DA', 'SS'])\n",
    "    for crew, seat, experiment in itertools.product(*choices):\n",
    "        query = (df.crew == crew)&(df.seat == seat)&(df.experiment == experiment)\n",
    "        thisdf = df[query][cols]\n",
    "        X_i, Y_i = to_sequences(thisdf.values, window_size)\n",
    "        X.append(X_i[:\n",
    "                     whats_proportion_index(\n",
    "                         X_i.shape[0],\n",
    "                         percent_of_data)])\n",
    "        Y.append(Y_i[:\n",
    "                     whats_proportion_index(\n",
    "                        Y_i.shape[0],\n",
    "                        percent_of_data)])\n",
    "        \n",
    "    return np.concatenate(X), np.concatenate(Y)\n",
    "\n",
    "# Borrowing parts of this func from \n",
    "# https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class10_lstm.ipynb\n",
    "def to_sequences(obs, seq_size, incols=[0], outcols=[1]):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(obs)-seq_size-1):\n",
    "        #print(i)\n",
    "        window = obs[i:(i+seq_size)][..., 0]\n",
    "        after_window = obs[i+seq_size, 1] # FIXME :off by 1 error here?\n",
    "        # window = [[x] for x in window]\n",
    "\n",
    "        x.append(window)\n",
    "        y.append(after_window)\n",
    "        \n",
    "    xarr = np.array(x)\n",
    "    yarr = np.array(y)\n",
    "    return (np.resize(xarr, xarr.shape + (1,)),\n",
    "            yarr)\n",
    "\n",
    "def bake_model(x_train, y_train, x_test, y_test, epochs=1, **kwargs):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2,\n",
    "                   input_shape=(None, 1)))\n",
    "    # model.add(Dense(32))\n",
    "\n",
    "    # 4 because 'A', 'B', 'C', 'D'.\n",
    "    model.add(Dense(4))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # initial_state ... for the LSTM , hmm\n",
    "\n",
    "\n",
    "    monitor = EarlyStopping(monitor='val_loss',\n",
    "                            min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "    print('Train...', timestamp())\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#arguments_8\n",
    "    # - hmm so fit() can take a generator sometimes.\n",
    "    # - use_multiprocessing=True \n",
    "    model.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
    "              callbacks=[monitor], verbose=2, epochs=epochs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def reshape_y(y, num_cols):\n",
    "\n",
    "    # y = np.array([1,2,3,2,3,1],dtype=np.int32)\n",
    "\n",
    "    # Convert y2 to dummy variables\n",
    "    y2 = np.zeros((y.shape[0], num_cols), dtype=np.float32)\n",
    "    y2[np.arange(y.shape[0]), y] = 1.0\n",
    "    return y2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
